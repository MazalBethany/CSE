{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "5d7b10c3-cc5f-4f28-86dd-2b075ccf4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models as tvmodels\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision.models as torchvisionmodels\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import LayerGradCam\n",
    "from captum.attr import visualization\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from dask_image.imread import imread\n",
    "from dask_image import ndfilters, ndmorph, ndmeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_image import ndmeasure\n",
    "\n",
    "from operator import itemgetter\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "d28e9f04-b41c-4497-b72d-72955325ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trainer import *\n",
    "\n",
    "allowed_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "labels_map = {\n",
    "    0: '0',\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6',\n",
    "    7: '7',\n",
    "    8: '8',\n",
    "    9: '9'\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# model = MNIST_model(len(allowed_classes))\n",
    "# checkpoint = torch.load('resnet_models/resnet18.pt')\n",
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "model_dict = torch.load('resnet_models/grad_cam_model.pt')\n",
    "model = gradcam_model()\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data, valid_data, test_data = create_dataloaders_MNIST(batch_size)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model and data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "cfc9b65b-403c-4762-a4a3-d68efd8d0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_img_transform = transforms.Normalize((0.1307,), (0.3081,))\n",
    "# This is to reverse the normalization done to the images that centered them around imagenet mean and std\n",
    "# The invTrans should be used on images before saving them.\n",
    "invTrans = transforms.Normalize((1/0.1307,), (1/0.3081,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "0334fe86-7945-4b86-ad21-9599388b54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f939d16e3a0>"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 0, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 5, 8, 9, 13, 16, 17, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "84dcbd51-01e7-4670-9290-0a6d4f80e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "from pytorch_grad_cam import XGradCAM, GradCAM, FullGrad, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from skimage.segmentation import slic, felzenszwalb, quickshift, watershed\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "43085c17-7287-4bd2-9a32-43fb2384b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayscale_grad_cam(image, SMU_class_index):\n",
    "    input_tensor = image.to(device)\n",
    "    targets = [ClassifierOutputTarget(SMU_class_index)]\n",
    "    #target_layers = [model.layer4[-1]]\n",
    "    target_layers = [model.layer2]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    return(grayscale_cam)\n",
    "\n",
    "def segmentation_info(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "# I want to get the average attribution score for each segment\n",
    "def cam_processor_for_segments(grayscale_cam_output, segments_slic):\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    region_attr_score = []\n",
    "    final_region_attr_score = []\n",
    "    num_pixels_in_region_list = []\n",
    "    \n",
    "    for i in (list_unique_regions):\n",
    "        row_counter = 0\n",
    "        column_counter = 0\n",
    "        region_attr_score = []\n",
    "        num_pixels_in_region = 0\n",
    "        for row in grayscale_cam_output:\n",
    "            for cell in row:\n",
    "                current_score = grayscale_cam_output[row_counter, column_counter]\n",
    "                current_region = segments_slic[row_counter, column_counter]\n",
    "                if current_region == i:\n",
    "                    region_attr_score.append(current_score)\n",
    "                    num_pixels_in_region += 1\n",
    "                column_counter +=1\n",
    "            row_counter += 1\n",
    "            column_counter = 0\n",
    "        avg_score = np.mean(region_attr_score)\n",
    "        final_region_attr_score.append(avg_score)\n",
    "        num_pixels_in_region_list.append(num_pixels_in_region)\n",
    "    \n",
    "    unique_region_info = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(final_region_attr_score[i-1])\n",
    "        image_list.append(num_pixels_in_region_list[i-1])\n",
    "        image_list.append(np.sum(num_pixels_in_region_list))\n",
    "        unique_region_info.append(image_list)\n",
    "    \n",
    "    return(unique_region_info)\n",
    "\n",
    "\n",
    "def get_feature_masks(image, attributions, segments_slic):\n",
    "    segments_slic_1 = segments_slic\n",
    "    features = []\n",
    "    for i in attributions:\n",
    "        feature = np.where(i==segments_slic_1, 1, 0)\n",
    "        features.append(feature)\n",
    "        \n",
    "    return(features)\n",
    "\n",
    "\n",
    "def attribution_ranker(cam_processor_for_segments_output, num_top_attr):\n",
    "    ranked_images = sorted(cam_processor_for_segments_output, key=itemgetter(1), reverse=True)\n",
    "    top_ranked_features = []\n",
    "    for i in range(num_top_attr):\n",
    "        top_ranked_features.append(ranked_images[i][0])\n",
    "        \n",
    "    return top_ranked_features\n",
    "\n",
    "\n",
    "\n",
    "def image_rankings(get_image_versions):\n",
    "    #for idx in iterative_Grad_CAM_counterfactual_masking_output\n",
    "    ranked_images = sorted(get_image_versions, key=itemgetter(3))\n",
    "    \n",
    "    return ranked_images\n",
    "\n",
    "def blur_image_from_attribution(image, attribution_map):\n",
    "    # attribution map is the attributions after being passed through the attribution processor\n",
    "    # image is a tensor\n",
    "    # will output the blurred image based on the attribution map\n",
    "    \n",
    "    \n",
    "    #average_img = image.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    #avg = np.average(average_img)\n",
    "    #blurred_img = cv2.GaussianBlur(image.squeeze().cpu().permute(1, 2, 0).numpy(), (181, 181), 0)\n",
    "    avg = np.float32(-0.4242)\n",
    "    #avg_img = np.where(average_img > 9999, average_img, avg)\n",
    "    \n",
    "    #attribution_map = attribution_map.detach().squeeze().cpu().numpy()\n",
    "    \n",
    "    mask = [attribution_map]\n",
    "    mask = np.array(mask).squeeze()\n",
    "\n",
    "    out = np.where(mask==np.array([0]), image.squeeze().cpu().numpy(), avg)\n",
    "\n",
    "    return torch.tensor(out).unsqueeze(0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "5bfe8a8e-6985-4874-beae-7986815658f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_info_slic(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "def segmentation_info_felzenszwalb(image, scale, sigma, min_size):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_felz = felzenszwalb(img_np, scale=scale, sigma=sigma, min_size=min_size)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "def softmax_score(num_total_pixels, num_obf_pixels, model, image, SMU_class_index):\n",
    "    #image = good_img_transform(image)\n",
    "    image = image\n",
    "    logits = model(image).cpu()\n",
    "    #print(logits)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    probs = probs[SMU_class_index]\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def region_explainability(image, top_n_start: int, model: torch.nn.Module, \n",
    "                          SMU_class_index, threshold: float, \n",
    "                          top_n_stop: int, MAX_BATCH_SZ: int = 16,\n",
    "                          PRUNE_HEURISTIC: int = 3):\n",
    "    if not(next(model.parameters()).is_cuda):\n",
    "        print('Model is not on GPU')\n",
    "        return -1\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, \n",
    "                                                 segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, \n",
    "                                   num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, \n",
    "                                   segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. \n",
    "    # Element at position 0 is the top attribution region mask\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.unravel_index(np.argmax(sm1), sm1.shape)[0]\n",
    "    prediction = sm_idx1 \n",
    "    confidence = sm1[sm_idx1]\n",
    "\n",
    "    powerset_list = [0] \n",
    "    total_images_analyzed = 0\n",
    "    searches_in_current_depth = 0\n",
    "    best_masked_image = None\n",
    "    start = perf_counter()\n",
    "    print('Starting with region search depth:', top_n)\n",
    "    features_list = features_1[0:top_n]\n",
    "    features_nums = list(range(len(features_list)))\n",
    "    powerset_list = list(more_itertools.powerset(features_nums))\n",
    "    powerset_list = [ele for ele in powerset_list if len(ele) != 0]\n",
    "    unique_image_info = []\n",
    "    #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "    num_pixels_changed = []\n",
    "    #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "    total_attr_list = []\n",
    "    #scores holds the score given to the image with regions obfuscated\n",
    "    scores = []\n",
    "    while True:\n",
    "        \n",
    "        # getting all combinations of features as a list, based on their index\n",
    "        if searches_in_current_depth == len(powerset_list):\n",
    "            top_n += 1\n",
    "            # PRUNE_HEURISTIC += 1\n",
    "            searches_in_current_depth = 0\n",
    "            features_list = features_1[0:top_n]\n",
    "            features_nums = list(range(len(features_list)))\n",
    "            powerset_list = list(more_itertools.powerset(features_nums))\n",
    "            powerset_list = [ele for ele in powerset_list if features_nums[-1] in ele]\n",
    "            # powerset_list = [ele for ele in powerset_list if len(ele) > PRUNE_HEURISTIC]\n",
    "            print('Number of images analyzed so far:', total_images_analyzed)\n",
    "            print('Increasing search depth to', top_n, 'regions\\n')        \n",
    "        \n",
    "        should_use_max_batch_size = MAX_BATCH_SZ <= len(powerset_list) - searches_in_current_depth\n",
    "        if should_use_max_batch_size:\n",
    "            batch_size = MAX_BATCH_SZ\n",
    "        else:\n",
    "            batch_size = len(powerset_list) - searches_in_current_depth\n",
    "        \n",
    "        image_tensor_batch = torch.zeros(batch_size, 1, 28, 28).to(device)\n",
    "        total_attribution = list()\n",
    "        num_changes = list()\n",
    "        total_num_pixels = list()\n",
    "        \n",
    "        for num in range(batch_size):\n",
    "            total_attribution.append(np.zeros((28, 28)))\n",
    "            num_changes.append(np.count_nonzero(total_attribution))\n",
    "            total_num_pixels.append(total_attribution[-1].size)\n",
    "            for i in range(len(powerset_list[searches_in_current_depth])):\n",
    "                total_attribution[num] += features_list[powerset_list[searches_in_current_depth][i]]\n",
    "            obfuscated_image = blur_image_from_attribution(image = image,\n",
    "                               attribution_map = total_attribution[num]).to(device)\n",
    "            image_tensor_batch[num] = obfuscated_image.detach().clone().squeeze(0)\n",
    "            searches_in_current_depth += 1\n",
    "            \n",
    "        np_output = model(image_tensor_batch).cpu().detach().numpy()\n",
    "        sm2 = np.apply_along_axis(softmax, 1, np_output)\n",
    "        sm_idx2 = np.unravel_index(np.argmax(sm2), sm2.shape)\n",
    "        img_index = sm_idx2[0]\n",
    "        cf_prediction = sm_idx2[1]\n",
    "        cf_confidence = sm2[sm_idx2]\n",
    "        total_images_analyzed += batch_size\n",
    "        if (prediction != cf_prediction) and (cf_confidence > threshold):\n",
    "            unique_image_info.append(image_tensor_batch[img_index])\n",
    "            unique_image_info.append(num_changes[img_index])\n",
    "            unique_image_info.append(total_num_pixels[img_index])\n",
    "            unique_image_info.append([confidence, cf_confidence])\n",
    "            unique_image_info.append([prediction, cf_prediction])\n",
    "            unique_image_info.append(total_attr_list)\n",
    "            unique_image_info.append(top_n)\n",
    "            unique_image_info.append(avg_attr_scores)\n",
    "            unique_image_info.append(total_images_analyzed)\n",
    "            print('Counterfactual found at depth:', top_n, 'regions')\n",
    "            print('Total Number of Counterfactuals tested:', total_images_analyzed)\n",
    "            end = perf_counter() - start\n",
    "            print(f'Total Search time: {end:.2f}')\n",
    "            break\n",
    "            \n",
    "        if top_n == top_n_stop and searches_in_current_depth == len(powerset_list):\n",
    "            print('Counterfactual not found up to depth (including):', top_n, 'regions')\n",
    "            print('Total Number of Counterfactuals tested:', total_images_analyzed)\n",
    "            end = perf_counter() - start\n",
    "            print(f'Total Search time: {end:.2f}')\n",
    "            return -1\n",
    "        \n",
    "        \n",
    "    return unique_image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "aa1a54de-3080-4618-bef6-62059c120e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93904a03d0>"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMhElEQVR4nO3dX6gc9RnG8efR1gvTXkTP6SHEpLYihtCL1IRQ8A+KtFhvkiCIAUMK2tOLWiL2opoiCqKW0j/0SjhB6YlYS8Gk5kLapEEIuQkeD2mMxlZbYkyI+edFrSKt5u3FmZSTuDt7sjOzs8n7/cBhd+fd2XkZ8mRm57e7P0eEAFz8Lmm7AQCDQdiBJAg7kARhB5Ig7EASXxjkxmxz6R9oWES40/JKR3bbt9v+m+13bD9U5bUANMv9jrPbvlTS3yV9W9JhSa9KWhsRb5asw5EdaFgTR/aVkt6JiH9GxH8k/V7SqgqvB6BBVcK+UNJ7sx4fLpadxfa47SnbUxW2BaCixi/QRcSEpAmJ03igTVWO7EckLZr1+KpiGYAhVCXsr0q61vbXbF8m6W5J2+ppC0Dd+j6Nj4hPbd8v6c+SLpX0bES8UVtnAGrV99BbXxvjPTvQuEY+VAPgwkHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLv+dklyfZBSR9K+kzSpxGxoo6mANSvUtgLt0bEyRpeB0CDOI0Hkqga9pC03fZrtsc7PcH2uO0p21MVtwWgAkdE/yvbCyPiiO2vSNoh6UcRsavk+f1vDMCcRIQ7La90ZI+II8XtcUlbJa2s8noAmtN32G3Ps/3lM/clfUfS/roaA1CvKlfjxyRttX3mdX4XEX+qpSsAtav0nv28N8Z7dqBxjbxnB3DhIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk6fnAyhdHR0a61EydONLrtefPmldbXrFnTV02SVq9eXVovvsLcVa9vTZat32vdW2+9tbS+a1fXH0VCBxzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnn6OGHH+5ae/DBB0vXLRujl3qPhW/YsKG0ft1113WtnTp1qnTdiYmJ0vrJk9Xm7Ny4cWPXWq9x9l6fAWCc/fxwZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJJjFtfDcc8+V1j/66KO+X/vmm28urV955ZWl9enp6dL61q1bu9Z6jaM37fHHH+9aKxuDl6RLLik/Fi1fvry03mu/Xaz6nsXV9rO2j9veP2vZFbZ32H67uJ1fZ7MA6jeX0/jfSrr9nGUPSdoZEddK2lk8BjDEeoY9InZJ+uCcxaskTRb3JyWtrrctAHXr97PxYxFxtLj/vqSxbk+0PS5pvM/tAKhJ5S/CRESUXXiLiAlJE9JwX6ADLnb9Dr0ds71Akorb4/W1BKAJ/YZ9m6T1xf31kl6qpx0ATek5zm77BUm3SBqRdEzSo5L+KOkPkhZLelfSXRFx7kW8Tq/V2ml8r++Mb968ubR++eWXd6312odPPfVUaX3Tpk2l9UOHDpXWh1nZftuzZ0/pukuXLi2tP/nkk6X1Rx55pLR+seo2zt7zPXtErO1Suq1SRwAGio/LAkkQdiAJwg4kQdiBJAg7kESan5LuNYzz1ltvldbLhse2bNlSum7Vn2O+kH388cdda5988knpur2+4joyMtJXT1lxZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJNKMsz/xxBOV6qjfgQMHSuvXX3/9gDrJgSM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiSRZpwdw2f37t2l9XvuuWdAneTAkR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUOr11TYOD89j+y2n7V93Pb+Wcses33E9t7i745m2wRQ1VxO438r6fYOy38dEcuKv5frbQtA3XqGPSJ2SfpgAL0AaFCVC3T3295XnObP7/Yk2+O2p2xPVdgWgIr6DfvTkq6RtEzSUUm/7PbEiJiIiBURsaLPbQGoQV9hj4hjEfFZRJyWtEnSynrbAlC3vsJue8Gsh2sk7e/2XADDoec4u+0XJN0iacT2YUmPSrrF9jJJIemgpB801yIuVjfddFNp3XZpvdf34XG2nmGPiLUdFj/TQC8AGsTHZYEkCDuQBGEHkiDsQBKEHUiCr7iiNUuWLCmt9/qKa68pn3E2juxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7Bha09PTleo4G0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcXY0anR0tGttZGSkdN2JiYm620mNIzuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4Oxq1fPnyrrXFixeXrnvq1Km620mt55Hd9iLbr9h+0/YbtjcUy6+wvcP228Xt/ObbBdCvuZzGfyrpxxGxVNK3JP3Q9lJJD0naGRHXStpZPAYwpHqGPSKORsR0cf9DSQckLZS0StJk8bRJSasb6hFADc7rPbvtqyV9U9IeSWMRcbQovS9prMs645LGK/QIoAZzvhpv+0uSXpT0QET8a3YtZmbg6zgLX0RMRMSKiFhRqVMAlcwp7La/qJmgPx8RW4rFx2wvKOoLJB1vpkUAdeh5Gm/bkp6RdCAifjWrtE3Sekk/K25faqRDXNAmJye71npNyYx6zeU9+w2S1kl63fbeYtlGzYT8D7bvlfSupLsa6RBALXqGPSJ2S3KX8m31tgOgKXxcFkiCsANJEHYgCcIOJEHYgSQ8yLFO2wysJnP69OmutRMnTpSuOzbW8RPY6CEiOo6ecWQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgST4KWlUsmTJktJ62ec4tmzZ0rWG+nFkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGdHJXfeeWdpfWbagc42bdpUdzsowZEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5KYy/zsiyRtljQmKSRNRMRvbD8m6fuSzvz498aIeLmpRtGO0dHR0vp9991XWi/7bfiTJ0/21RP6M5cP1Xwq6ccRMW37y5Jes72jqP06In7RXHsA6jKX+dmPSjpa3P/Q9gFJC5tuDEC9zus9u+2rJX1T0p5i0f2299l+1vb8LuuM256yPVWtVQBVzDnstr8k6UVJD0TEvyQ9LekaScs0c+T/Zaf1ImIiIlZExIrq7QLo15zCbvuLmgn68xGxRZIi4lhEfBYRpyVtkrSyuTYBVNUz7J752tIzkg5ExK9mLV8w62lrJO2vvz0AdZnL1fgbJK2T9LrtvcWyjZLW2l6mmeG4g5J+0EB/aNnixYsr1bdv3961dujQob56Qn/mcjV+t6ROX0pmTB24gPAJOiAJwg4kQdiBJAg7kARhB5Ig7EAS/JQ0KimbklmS1q1bN6BO0AtHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Iwr3GSWvdmH1C0ruzFo1IGtbfEx7W3oa1L4ne+lVnb1+NiI6//z3QsH9u4/bUsP423bD2Nqx9SfTWr0H1xmk8kARhB5JoO+wTLW+/zLD2Nqx9SfTWr4H01up7dgCD0/aRHcCAEHYgiVbCbvt223+z/Y7th9rooRvbB22/bntv2/PTFXPoHbe9f9ayK2zvsP12cdtxjr2WenvM9pFi3+21fUdLvS2y/YrtN22/YXtDsbzVfVfS10D228Dfs9u+VNLfJX1b0mFJr0paGxFvDrSRLmwflLQiIlr/AIbtmyX9W9LmiPhGseznkj6IiJ8V/1HOj4ifDElvj0n6d9vTeBezFS2YPc24pNWSvqcW911JX3dpAPutjSP7SknvRMQ/I+I/kn4vaVULfQy9iNgl6YNzFq+SNFncn9TMP5aB69LbUIiIoxExXdz/UNKZacZb3XclfQ1EG2FfKOm9WY8Pa7jmew9J222/Znu87WY6GIuIo8X99yWNtdlMBz2n8R6kc6YZH5p918/051Vxge7zboyI6yV9V9IPi9PVoRQz78GGaex0TtN4D0qHacb/r8191+/051W1EfYjkhbNenxVsWwoRMSR4va4pK0avqmoj52ZQbe4Pd5yP/83TNN4d5pmXEOw79qc/ryNsL8q6VrbX7N9maS7JW1roY/PsT2vuHAi2/MkfUfDNxX1Nknri/vrJb3UYi9nGZZpvLtNM66W913r059HxMD/JN2hmSvy/5D00zZ66NLX1yX9tfh7o+3eJL2gmdO6/2rm2sa9kq6UtFPS25L+IumKIertOUmvS9qnmWAtaKm3GzVzir5P0t7i7462911JXwPZb3xcFkiCC3RAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kMT/AC9Q8EoPK94eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 26, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 0, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "e68775b9-cda0-47e9-ae96-cb31617d6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "index: 17\n",
      "index: 26\n",
      "index: 34\n",
      "index: 36\n",
      "index: 41\n",
      "index: 60\n",
      "index: 64\n",
      "index: 70\n",
      "index: 75\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "n = 0\n",
    "while i < 10:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    \n",
    "    if just_label == 7 and predicted == 7:\n",
    "        print('index:', n)\n",
    "        i += 1\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "b73904f2-f297-44c2-b772-95921e51ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93902bb2b0>"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMX0lEQVR4nO3dT6xcdRnG8ecRdQMsWolNA1XQsDEmVu/YGGmMxkCwm+KGyMKAIb1dSICERBuM2rBqVDSsCBclFKMYEyWyMGptTCouSKek0j8oICmxzaVFu7CuEHhdzCkZ2jtzbuf8nft+P8nNzJxzZs57T+/Tc+b8zu/8HBECsPa9p+sCALSDsANJEHYgCcIOJEHYgSTe2+bKbPf21P/CwuzvPXSovjpmMa+1V6lbovZJIsIrTXeVpjfbN0t6SNJlkn4cEXtKlu9t2Ku0QHrFTdueea29aqsvta+s9rDbvkzSi5JulHRS0kFJt0XE8SnvIewNmNfa+xyYMn2ufVLYq3xn3yLp5Yh4JSLekPQLSdsrfB6ABlUJ+9WS/jn2+mQx7V1sL9oe2h5WWBeAiho/QRcRS5KWpH4fxgNrXZU9+ylJm8ZeX1NMA9BDVcJ+UNL1tq+z/X5JX5H0dD1lAajbzIfxEfGm7bsk/V6jprfHIuJYbZUBqFWldvZLXlmPv7PPa/OVNL+197n5qkyfa2+i6Q3AHCHsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiZmHbJ7FwoI0HLa5xna0OBAuxszzdm+q9sFg8rxKYbd9QtI5SW9JejMipqwKQJfq2LN/ISL+VcPnAGgQ39mBJKqGPST9wfYh24srLWB70fbQ9vD11yuuDcDMqh7Gb42IU7Y/KGmf7b9FxIHxBSJiSdKSJA0GnuNTKsB8q7Rnj4hTxeMZSU9J2lJHUQDqN3PYbV9u+8rzzyXdJOloXYUBqFeVw/gNkp6yff5zfh4Rv6ulKgC1mznsEfGKpE/UWAuABtH0BiRB2IEkCDuQBGEHkiDsQBKOFvsJ2v29gq7KZhi1PvbTPHcDbVLZv1nV7dbl30RErLh29uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESrt5IGLk1ZY3ePL3DoIfbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEmnb26v26J39A2Wfv3Llz6vxHHnlkloLGNNne3GWH+Gq/V5/vM9AF9uxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESa+8Y32c7evLXajl6mu3b2lPeNt/2Y7TO2j45NW297n+2Xisd1dRYLoH6rOYx/XNLNF0zbJWl/RFwvaX/xGkCPlYY9Ig5IOnvB5O2S9hbP90q6pd6yANRt1mvjN0TEcvH8NUkbJi1oe1HS4ozrAVCTyh1hIiKmnXiLiCVJS1K/B3YE1rpZm95O294oScXjmfpKAtCEWcP+tKTbi+e3S/pNPeUAaErpYbztJyV9XtJVtk9K+q6kPZJ+aftOSa9KunU1K1tYkIbD2Yvt1lrtHD399/rOd749df4DDzzQ2Lqr6nJs+q7WPRhMnlca9oi4bcKsL85YD4AOcLkskARhB5Ig7EAShB1IgrADSbTaxXUwcMxv0xvQf4OBNBzO2MUVwNpA2IEkCDuQBGEHkiDsQBKEHUiCsANJpLmVdJkqm2GehwbushtomW5vx1zt/XN5K2kAawNhB5Ig7EAShB1IgrADSRB2IAnCDiRReUQYZFfWID3HFyGsMezZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ2tnnQLd9zquufNr7p7fBN/l7f+1rd5Qs8XhzK+9I6Z7d9mO2z9g+OjZtt+1Ttg8XP9uaLRNAVas5jH9c0s0rTP9RRGwufn5bb1kA6lYa9og4IOlsC7UAaFCVE3R32X6+OMxfN2kh24u2h7YZ5Q3o0Kxhf1jSRyVtlrQs6cFJC0bEUkQMImIw47oA1GCmsEfE6Yh4KyLelvSopC31lgWgbjOF3fbGsZdflnR00rIA+qH0vvG2n5T0eUlXSTot6bvF680aNaKekLQzIpZLV8Z942tfd3W9/SfRPPeF7+N94xkkokDY+4iwz4JBIoDkCDuQBGEHkiDsQBKEHUii1S6uCwvScA1eNNvnYY/LlZ02bvKXm9+z7WW6+psYTLlOlT07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBraRRomo7/NptS5837NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnuLlvo8u6yVc1rf/qut9s0Vbcpd5cF0BnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC/uzJ/eUvz0ydf8MNW1uqBE0r3bPb3mT7T7aP2z5m+55i+nrb+2y/VDyua75cALNazWH8m5Lui4iPSfqMpK/b/pikXZL2R8T1kvYXrwH0VGnYI2I5Ip4rnp+T9IKkqyVtl7S3WGyvpFsaqhFADS7pO7vtayV9UtKzkjZExHIx6zVJGya8Z1HSYoUaAdRg1WfjbV8h6VeS7o2I/4zPi1FvmhW7DkTEUkQMImLKkHMAmraqsNt+n0ZB/1lE/LqYfNr2xmL+RklnmikRQB1Ku7jatkbfyc9GxL1j078v6d8Rscf2LknrI+IbJZ/V286Yebu4Vv0nmf2X73q7TbMWu7iuJuxbJf1Z0hFJbxeT79foe/svJX1I0quSbo2IsyWfRdgbQNjrlzLsdSLszSDs9VuLYedyWSAJwg4kQdiBJAg7kARhB5Kgiys6M6+3wF6Nrn63wZTrVNmzA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASrbazLyxIw2Gba2zHfLcXN9s9a1rvr/nebvOHPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEF/dsytJu/gOs93l52EPTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJFHazm57k6QnJG3QaMjPpYh4yPZuSTskvV4sen9E/HbaZx061M/2RynzKK7VDIcHS5b4dCt1oNxqLqp5U9J9EfGc7SslHbK9r5j3o4j4QXPlAahLadgjYlnScvH8nO0XJF3ddGEA6nVJ39ltXyvpk5KeLSbdZft524/ZXjfhPYu2h7bX4A2pgPmx6rDbvkLSryTdGxH/kfSwpI9K2qzRnv/Bld4XEUsRMYiIKaNQAWjaqsJu+30aBf1nEfFrSYqI0xHxVkS8LelRSVuaKxNAVaVht21JP5H0QkT8cGz6xrHFvizpaP3lAajLas7G3yDpq5KO2D5cTLtf0m22N2vUHHdC0s4G6kPDHn10aer8HTt2TJ1/992frbMcNGg1Z+Of0co3F5/apg6gX7iCDkiCsANJEHYgCcIOJEHYgSQIO5CEo8X+kbZ7O0hv1i6uXdbe59s197m2MhGx4trZswNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEm0P2fwvSa+Ovb6qmNa5FdpFe1PbBS6qq+t2/jGXtM1arnvN1Fbiw5NmtHpRzUUrt4d9vTddX2vra10Stc2qrdo4jAeSIOxAEl2HffoN0LrV19r6WpdEbbNqpbZOv7MDaE/Xe3YALSHsQBKdhN32zbb/bvtl27u6qGES2ydsH7F9uOvx6Yox9M7YPjo2bb3tfbZfKh5XHGOvo9p22z5VbLvDtrd1VNsm23+yfdz2Mdv3FNM73XZT6mplu7X+nd32ZZJelHSjpJOSDkq6LSKOt1rIBLZPSBpEROcX1Nj+nKT/SnoiIj5eTPuepLMRsaf4j3JdRHyzJ7XtlvTfrofxLkYr2jg+zLikWyTdoQ633ZS6blUL262LPfsWSS9HxCsR8YakX0ja3kEdvRcRBySdvWDydkl7i+d7Nfpjad2E2nohIpYj4rni+TlJ54cZ73TbTamrFV2E/WpJ/xx7fVL9Gu89JP3B9iHbi10Xs4INEbFcPH9N0oYui1lB6TDebbpgmPHebLtZhj+vihN0F9saEZ+S9CVJXy8OV3spRt/B+tR2uqphvNuywjDj7+hy2806/HlVXYT9lKRNY6+vKab1QkScKh7PSHpK/RuK+vT5EXSLxzMd1/OOPg3jvdIw4+rBtuty+PMuwn5Q0vW2r7P9fklfkfR0B3VcxPblxYkT2b5c0k3q31DUT0u6vXh+u6TfdFjLu/RlGO9Jw4yr423X+fDnEdH6j6RtGp2R/4ekb3VRw4S6PiLpr8XPsa5rk/SkRod1/9Po3Madkj4gab+klyT9UdL6HtX2U0lHJD2vUbA2dlTbVo0O0Z+XdLj42db1tptSVyvbjctlgSQ4QQckQdiBJAg7kARhB5Ig7EAShB1IgrADSfwfhKQ6ph+2oRkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "7ec91c4a-f5a8-47df-880e-fa8ef3e2f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_example = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "39d1964c-c910-479b-89f9-dc56a3a40c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93928af670>"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 60, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens:index: 0, 17, 26, 34, 36, 41, 60, 64, 70, 75\n",
    "# Good outputs 17, 48, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "cc66067e-f097-45ba-9716-46abc5f87595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f93a38ccf40>"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.min())\n",
    "plt.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "e79a2508-c955-4c57-8d5e-2be0f67c98aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "Starting with region search depth: 4\n",
      "Number of images analyzed so far: 15\n",
      "Increasing search depth to 5 regions\n",
      "\n",
      "Number of images analyzed so far: 31\n",
      "Increasing search depth to 6 regions\n",
      "\n",
      "Number of images analyzed so far: 63\n",
      "Increasing search depth to 7 regions\n",
      "\n",
      "Number of images analyzed so far: 127\n",
      "Increasing search depth to 8 regions\n",
      "\n",
      "Number of images analyzed so far: 255\n",
      "Increasing search depth to 9 regions\n",
      "\n",
      "Number of images analyzed so far: 511\n",
      "Increasing search depth to 10 regions\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counterfactual found at depth: 10 regions\n",
      "Total Number of Counterfactuals tested: 783\n",
      "Total Search time: 0.14\n",
      "regions analyzed 10\n",
      "Original Version Predicted Class: 7      With Confidence: 0.9999999\n",
      "Modified Version Predicted Class: 1      With Confidence: 0.9956826\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just give an images and it will output\n",
    "\n",
    "index = 0\n",
    "class_target = 7\n",
    "top_n_start = 4\n",
    "top_n_stop = 20\n",
    "threshold = 0.92\n",
    "# pruning_heurist is disabled atm, it almost works!\n",
    "pruning_heuristic = 1\n",
    "# batch size of 16 appears to be optimal for search speed\n",
    "# can do 20 regions in ~3 minutes!\n",
    "batch_sz = 16\n",
    "images, labels = next(itertools.islice(test_data, index, None))\n",
    "\n",
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(class_target)]\n",
    "target_layers = [model.layer2]\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "inv_img = images\n",
    "print(inv_img.shape)\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "\n",
    "fast_model = torch.jit.trace(model.to(device), torch.zeros(batch_sz, 1, 28, 28).to(device))\n",
    "\n",
    "working_example = region_explainability(image = images, top_n_start = top_n_start, \n",
    "                                        model = fast_model, SMU_class_index = class_target, \n",
    "                                        threshold = threshold, top_n_stop = top_n_stop,\n",
    "                                        MAX_BATCH_SZ = batch_sz,\n",
    "                                        PRUNE_HEURISTIC = pruning_heuristic)\n",
    "\n",
    "ori_prediction = working_example[4][0]\n",
    "ori_confidence = working_example[3][0]\n",
    "cf_prediction = working_example[4][1]\n",
    "cf_confidence = working_example[3][1]\n",
    "\n",
    "print(\"regions analyzed\", working_example[-3])\n",
    "print(\"Original Version Predicted Class:\", ori_prediction, \n",
    "      \"     With Confidence:\", ori_confidence)\n",
    "print(\"Modified Version Predicted Class:\", cf_prediction, \n",
    "      \"     With Confidence:\", cf_confidence)\n",
    "\n",
    "plot_images = (images.detach().cpu().squeeze(),\n",
    "              grayscale_cam,\n",
    "              segmentation.mark_boundaries(img_np, segments_slic),\n",
    "              working_example[0].detach().cpu().squeeze())\n",
    "\n",
    "figure_name = plt.figure(figsize=(16, 16))\n",
    "for i, img in enumerate(plot_images):\n",
    "        plt.subplot(1, 4,i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img, cmap='gray',interpolation='none')\n",
    "figure_name.savefig('test.png')\n",
    "\n",
    "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "# fig.figure(figsize=(12, 12))\n",
    "# ax1.imshow(images.detach().cpu().squeeze(), cmap='gray')\n",
    "# ax1.axis('off')\n",
    "# ax2.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)\n",
    "# ax2.axis('off')\n",
    "# ax3.imshow(segmentation.mark_boundaries(img_np, segments_slic))\n",
    "# ax3.axis('off')\n",
    "# ax4.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')\n",
    "# ax4.axis('off')\n",
    "\n",
    "# fig.savefig('test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964de447-2c49-4819-9783-95943389ecac",
   "metadata": {},
   "source": [
    "# Quantitative Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30cc76-24d6-4a87-808a-12501799d81a",
   "metadata": {},
   "source": [
    "## Experimenting with different clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c737dd-66be-48ea-8bfc-2f3ff778abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            if sm_idx1 != sm_idx2:\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    print()\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ea514-1555-436c-aa39-76858a9cf280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "3\n",
      "index: 2\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "1\n",
      "index: 3\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n",
      "Regions analyzed 12\n",
      "Regions analyzed 13\n",
      "Regions analyzed 14\n",
      "Regions analyzed 15\n",
      "Regions analyzed 16\n"
     ]
    }
   ],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "i = 0\n",
    "n = 0\n",
    "image_info_list = []\n",
    "while i < 50:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    #n += 1\n",
    "    #print()\n",
    "    #print(predicted)\n",
    "    \n",
    "    logits = model(images.to(device)).cpu()\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    # Change probs[int] to int = SMU class index\n",
    "    probs_orig = probs[0]\n",
    "    #print(probs)\n",
    "    \n",
    "    \n",
    "    if just_label == 7 and predicted ==7:\n",
    "        print('index:', i+1)\n",
    "        i += 1\n",
    "        \n",
    "        re = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 7)\n",
    "        \n",
    "        \n",
    "        image_info = []\n",
    "        example = re[0]\n",
    "        exam_img = good_img_transform(example)\n",
    "        logits = model(exam_img).cpu()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = probs.detach().cpu()\n",
    "        probs = probs.tolist()[0]\n",
    "        # Change probs[int] to int = SMU class index\n",
    "        probs_obf = probs[0]\n",
    "        #print(probs)\n",
    "        image_info.append(probs_orig)\n",
    "        image_info.append(probs_obf)\n",
    "        \n",
    "        num_pixels_obf = re[1]\n",
    "        image_info.append(num_pixels_obf)\n",
    "        image_info.append(re[5])\n",
    "        sm1 = softmax(model(example.to(device)).cpu().detach().numpy()).squeeze()\n",
    "        sm_idx1 = np.argmax(sm1)\n",
    "        #sm_idx1 is the predicted class of the obfuscated image\n",
    "        image_info.append(sm_idx1)\n",
    "        \n",
    "        image_info_list.append(image_info)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b12456-0462-4376-ba6c-ce7a3fd69479",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "total = len(image_info_list)\n",
    "total_pix = total * 28 * 28\n",
    "total_obf = 0\n",
    "total_orig_conf = 0\n",
    "total_obf_conf = 0\n",
    "total_regions = 0\n",
    "preds_on_images = []\n",
    "for i in range(len(image_info_list)):\n",
    "    total_orig_conf += image_info_list[i][0]\n",
    "    total_obf_conf += image_info_list[i][1]\n",
    "    total_obf += image_info_list[i][2]\n",
    "    total_regions += image_info_list[i][3]\n",
    "    preds_on_images.append(image_info_list[i][4])\n",
    "\n",
    "\n",
    "array_np = np.array(preds_on_images)\n",
    "unique, counts = np.unique(array_np, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "\n",
    "print(\"Experiment with SLIC with 25 segments and 1 compactness\")\n",
    "print(\"Average number of regions analyzed: \", total_regions / total)\n",
    "print(\"Average confidence change: \", ((total_orig_conf - total_obf_conf) / total) )\n",
    "print(\"Distribution of changed to class: \", dict(zip(unique, counts)))\n",
    "print(\"Average obfuscation: \",total_obf / total_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf22ea-15a9-4b6d-96b4-a2633eec562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with felz with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "\n",
    "    while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "                                  num_obf_pixels = num_pixels_changed,\n",
    "                                  model = model,\n",
    "                                  image = obfuscated_image,\n",
    "                                  SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            if score < 0.5:\n",
    "                prob = score\n",
    "                image_versions.append(obfuscated_image)\n",
    "                scores.append(score)\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "                \n",
    "                #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    \n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db9528-ab90-4a3c-b634-f49274165c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
