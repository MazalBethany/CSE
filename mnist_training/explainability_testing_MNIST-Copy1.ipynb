{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7b10c3-cc5f-4f28-86dd-2b075ccf4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models as tvmodels\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision.models as torchvisionmodels\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import LayerGradCam\n",
    "from captum.attr import visualization\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from dask_image.imread import imread\n",
    "from dask_image import ndfilters, ndmorph, ndmeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_image import ndmeasure\n",
    "\n",
    "from operator import itemgetter\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28e9f04-b41c-4497-b72d-72955325ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trainer import *\n",
    "\n",
    "allowed_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "labels_map = {\n",
    "    0: '0',\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6',\n",
    "    7: '7',\n",
    "    8: '8',\n",
    "    9: '9'\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# model = MNIST_model(len(allowed_classes))\n",
    "# checkpoint = torch.load('resnet_models/resnet18.pt')\n",
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "model_dict = torch.load('resnet_models/grad_cam_model.pt')\n",
    "model = gradcam_model()\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data, valid_data, test_data = create_dataloaders_MNIST(batch_size)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model and data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc9b65b-403c-4762-a4a3-d68efd8d0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_img_transform = transforms.Normalize((0.1307,), (0.3081,))\n",
    "# This is to reverse the normalization done to the images that centered them around imagenet mean and std\n",
    "# The invTrans should be used on images before saving them.\n",
    "invTrans = transforms.Normalize((1/0.1307,), (1/0.3081,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0334fe86-7945-4b86-ad21-9599388b54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb07073a1c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 0, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 5, 8, 9, 13, 16, 17, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dcbd51-01e7-4670-9290-0a6d4f80e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "from pytorch_grad_cam import XGradCAM, GradCAM, FullGrad, GradCAMPlusPlus, ScoreCAM, AblationCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from skimage.segmentation import slic, felzenszwalb, quickshift, watershed, flood\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43085c17-7287-4bd2-9a32-43fb2384b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayscale_grad_cam(image, SMU_class_index):\n",
    "    input_tensor = image.to(device)\n",
    "    targets = [ClassifierOutputTarget(SMU_class_index)]\n",
    "    #target_layers = [model.layer4[-1]]\n",
    "    target_layers = [model.layer2]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    return(grayscale_cam)\n",
    "\n",
    "def segmentation_info(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "# I want to get the average attribution score for each segment\n",
    "def cam_processor_for_segments(grayscale_cam_output, segments_slic):\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    region_attr_score = []\n",
    "    final_region_attr_score = []\n",
    "    num_pixels_in_region_list = []\n",
    "    \n",
    "    for i in (list_unique_regions):\n",
    "        row_counter = 0\n",
    "        column_counter = 0\n",
    "        region_attr_score = []\n",
    "        num_pixels_in_region = 0\n",
    "        for row in grayscale_cam_output:\n",
    "            for cell in row:\n",
    "                current_score = grayscale_cam_output[row_counter, column_counter]\n",
    "                current_region = segments_slic[row_counter, column_counter]\n",
    "                if current_region == i:\n",
    "                    region_attr_score.append(current_score)\n",
    "                    num_pixels_in_region += 1\n",
    "                column_counter +=1\n",
    "            row_counter += 1\n",
    "            column_counter = 0\n",
    "        avg_score = np.mean(region_attr_score)\n",
    "        final_region_attr_score.append(avg_score)\n",
    "        num_pixels_in_region_list.append(num_pixels_in_region)\n",
    "    \n",
    "    unique_region_info = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(final_region_attr_score[i-1])\n",
    "        image_list.append(num_pixels_in_region_list[i-1])\n",
    "        image_list.append(np.sum(num_pixels_in_region_list))\n",
    "        unique_region_info.append(image_list)\n",
    "    \n",
    "    return(unique_region_info)\n",
    "\n",
    "\n",
    "def get_feature_masks(image, attributions, segments_slic):\n",
    "    segments_slic_1 = segments_slic\n",
    "    features = []\n",
    "    for i in attributions:\n",
    "        feature = np.where(i==segments_slic_1, 1, 0)\n",
    "        features.append(feature)\n",
    "        \n",
    "    return(features)\n",
    "\n",
    "\n",
    "def attribution_ranker(cam_processor_for_segments_output, num_top_attr):\n",
    "    ranked_images = sorted(cam_processor_for_segments_output, key=itemgetter(1), reverse=True)\n",
    "    top_ranked_features = []\n",
    "    for i in range(num_top_attr):\n",
    "        top_ranked_features.append(ranked_images[i][0])\n",
    "        \n",
    "    return top_ranked_features\n",
    "\n",
    "\n",
    "\n",
    "def image_rankings(get_image_versions):\n",
    "    #for idx in iterative_Grad_CAM_counterfactual_masking_output\n",
    "    ranked_images = sorted(get_image_versions, key=itemgetter(3))\n",
    "    \n",
    "    return ranked_images\n",
    "\n",
    "def blur_image_from_attribution(image, attribution_map):\n",
    "    # attribution map is the attributions after being passed through the attribution processor\n",
    "    # image is a tensor\n",
    "    # will output the blurred image based on the attribution map\n",
    "    \n",
    "    \n",
    "    #average_img = image.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    #avg = np.average(average_img)\n",
    "    #blurred_img = cv2.GaussianBlur(image.squeeze().cpu().permute(1, 2, 0).numpy(), (181, 181), 0)\n",
    "    avg = np.float32(-0.4242)\n",
    "    #avg_img = np.where(average_img > 9999, average_img, avg)\n",
    "    \n",
    "    #attribution_map = attribution_map.detach().squeeze().cpu().numpy()\n",
    "    \n",
    "    mask = [attribution_map]\n",
    "    mask = np.array(mask).squeeze()\n",
    "    #mask = mask.transpose(1,2,0)\n",
    "    #print(mask.shape)\n",
    "    #print(image.squeeze().cpu().numpy().shape)\n",
    "    \n",
    "    \n",
    "    out = np.where(mask==np.array([0]), image.squeeze().cpu().numpy(), avg)\n",
    "    #out = np.where(mask==np.array([0, 0, 0]), image.squeeze().cpu().permute(1, 2, 0).numpy(), blurred_img)\n",
    "    \n",
    "    out = torch.tensor(out)\n",
    "    out = out\n",
    "    out = out.unsqueeze(0)\n",
    "    out = out.unsqueeze(0)\n",
    "    #print(out.shape)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfe8a8e-6985-4874-beae-7986815658f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_info_slic(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "def segmentation_info_felzenszwalb(image, scale, sigma, min_size):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = felzenszwalb(img_np, scale=scale, sigma=sigma, min_size=min_size)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "def segmentation_info_quickshift(image, kernel_size, max_dist, ratio):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = quickshift(img_np, kernel_size, max_dist, ratio)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "def segmentation_info_bass(segmentation_dir):\n",
    "    #img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = quickshift(img_np, kernel_size, max_dist, ratio)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "def softmax_score(num_total_pixels, num_obf_pixels, model, image, SMU_class_index):\n",
    "    #image = good_img_transform(image)\n",
    "    image = image\n",
    "    logits = model(image).cpu()\n",
    "    #print(logits)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    probs = probs[SMU_class_index]\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index, threshold, top_n_stop):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "    print(len(features_1))\n",
    "    \n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    best_score_other_class = 0\n",
    "    \n",
    "    previous_features = False\n",
    "    powerset_list = list()\n",
    "    start = perf_counter()\n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "\n",
    "\n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        # print(type(features_list[-1]))\n",
    "        if previous_features:\n",
    "            powerset_list = np.array([list(ele) for ele in powerset_list if len(ele) != 0], dtype=object)\n",
    "            for ele in powerset_list:\n",
    "                for i, mask in enumerate(ele[:]):\n",
    "                    if np.array_equal(mask, features_list[-1]):\n",
    "                        ele = np.delete(mask, i, axis=0)\n",
    "        else:\n",
    "            powerset_list = [list(ele) for ele in powerset_list if len(ele) != 0]\n",
    "        \n",
    "        \n",
    "        # print(len(powerset_list))\n",
    "        \n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "    \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            \n",
    "            if (sm_idx1 != sm_idx2) and sm2[sm_idx2] > best_score_other_class:\n",
    "                best_score_other_class = sm2[sm_idx2] \n",
    "            \n",
    "            if (sm_idx1 != sm_idx2) and (sm2[sm_idx2] > threshold):\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "        if top_n == top_n_stop:\n",
    "            end = perf_counter() - start\n",
    "            print(f'Total Search time: {end:.3f}')\n",
    "            return -1\n",
    "        previous_features = True\n",
    "    \n",
    "    end = perf_counter() - start\n",
    "    print(f'Total Search time: {end:.3f}')\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1a54de-3080-4618-bef6-62059c120e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcabd9dd520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 26, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 0, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e68775b9-cda0-47e9-ae96-cb31617d6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 7\n",
      "index: 9\n",
      "index: 12\n",
      "index: 16\n",
      "index: 20\n",
      "index: 58\n",
      "index: 62\n",
      "index: 73\n",
      "index: 78\n",
      "index: 92\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "n = 0\n",
    "while i < 10:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    \n",
    "    if just_label == 9 and predicted == 9:\n",
    "        print('index:', n)\n",
    "        i += 1\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64475f9b-7cbe-47a5-8a92-0a9ad2b81118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n",
      "tensor([9], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f57941c1fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 58, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 5, 8, 9, 13, 16, 17, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73904f2-f297-44c2-b772-95921e51ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 6  6  6  6  6  6  2  2  2  2  2  2  3  3  3  3  3  3  7  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  6  9  9  9  9  9  9  3  3  3  3  3  7  7  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  6  9  9  9  9  9  9 10 10 10 10 10 10  7  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  6  9  9  9  9  9 10 10 10 10 10 10 10 10  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  9  9  9  9  9  9  9 11 11 11 11 10 10 10  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  9  9  9  9  9  9 11 11 11 11 11 11 10 10  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [12 12 12 12 12  9  9 13 13 13 13 11 11 11 11 11 11 14 14 14  7 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 11 11 11 11 11 11 14 14 14 15 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 11 11 11 11 11 14 14 14 15 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 11 11 11 11 11 14 14 14 15 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 11 11 11 11 11 14 14 14 16 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 17 17 17 17 17 14 14 16 16 16 16 15\n",
      "  15 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 17 14 14 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 17 14 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 17 21 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 21 21 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 20 17 17 17 17 21 21 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 22 22 20 20 23 23 23 23 23 17 17 21 21 21 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 21 21 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 21 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 21 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 24 24 24 24 24 24 24 25\n",
      "  25 25 25 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f57945a2b20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "print(segments_slic)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34c216e-cdc1-43b5-abe0-e7c33c5709a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5794511340>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "print(img_np.shape)\n",
    "segments_slic = watershed(img_np, markers=25, compactness=0.001)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe1c72d-737b-42ab-8236-4db6547ab334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f578d799bb0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = felzenszwalb(img_np, scale=5, sigma=0.5, min_size=5)\n",
    "#print(segments_slic)\n",
    "print(len(np.unique(segments_slic)))\n",
    "print(np.unique(segments_slic))\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9699755-2fa8-4950-a5c7-ce00399f9c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the input array must have size 3 along `channel_axis`, got (28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20864/3152609198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.imshow(img_np)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compactness=50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msegments_slic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquickshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(segments_slic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_slic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/segmentation/_quickshift.py\u001b[0m in \u001b[0;36mquickshift\u001b[0;34m(image, ratio, kernel_size, max_dist, return_tree, sigma, convert2lab, random_seed, channel_axis)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only RGB images can be converted to Lab space.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2lab\u001b[0;34m(rgb, illuminant, observer, channel_axis)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mStandard_illuminant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \"\"\"\n\u001b[0;32m-> 1139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxyz2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milluminant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2xyz\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m# Follow the algorithm from http://www.easyrgb.com/index.php\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# except we don't multiply/divide by 100 in the conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_colorarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.04045\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1.055\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36m_prepare_colorarray\u001b[0;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[1;32m    138\u001b[0m         msg = (f'the input array must have size 3 along `channel_axis`, '\n\u001b[1;32m    139\u001b[0m                f'got {arr.shape}')\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mfloat_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_supported_float_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the input array must have size 3 along `channel_axis`, got (28, 28, 1)"
     ]
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = quickshift(img_np, kernel_size=1, max_dist=6, ratio=0.5)\n",
    "#print(segments_slic)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6be1949-6963-4c67-9ccd-a1525bff5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_mask_to_numpy(csv_path: str) -> np.ndarray:\n",
    "    \"\"\"Converts a csv_mask_file into a numpy arrays.\n",
    "    \n",
    "    Args:\n",
    "        csv_mask_dir_path (str): The path to csv mask directory.\n",
    "            Note, this should be the same as 'output_dir_path' from run_bass() function\n",
    "        \n",
    "    Returns:\n",
    "        A list of np array masks.\n",
    "    \"\"\"\n",
    "    # - 1 so that the superpixel values start at 0\n",
    "    return np.loadtxt(csv_path, delimiter=\",\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501a0929-e6b2-4cde-9443-5494e3dad4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n"
     ]
    }
   ],
   "source": [
    "a = \"test.png\"\n",
    "b = a[:-3] + 'csv'\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "114db533-d541-4d36-972c-3943b9ccdce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0da49ce580>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = \"../labelme/MNIST_71/test_images\"\n",
    "img = Image.open(str(data_dir)+\"/seven_170.png\")\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        # transforms.RandomAffine(20, translate=(0.20,0.20))\n",
    "    ])\n",
    "image_to_tensor = transform_test(img).unsqueeze(0)\n",
    "img_np = image_to_tensor.detach().cpu().squeeze().numpy()\n",
    "bass_seg = csv_mask_to_numpy(\"../labelme/MNIST_71/BASS_output/seven_170.csv\")\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, bass_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec91c4a-f5a8-47df-880e-fa8ef3e2f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_example = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079ce5a4-26e3-4d45-ac78-d2b727281400",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'working_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13967/3530658002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'working_example' is not defined"
     ]
    }
   ],
   "source": [
    "print(working_example[-2])\n",
    "print(working_example[-1])\n",
    "print(model(images.to(device)))\n",
    "print(model(working_example[0].to(device)))\n",
    "plt.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39d1964c-c910-479b-89f9-dc56a3a40c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0da517baf0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 60, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens:index: 0, 17, 26, 34, 36, 41, 60, 64, 70, 75\n",
    "# Good outputs 17, 48, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc66067e-f097-45ba-9716-46abc5f87595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14731/1627894948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mClassifierOutputTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "input_tensor = images.to(device)\n",
    "print(input_tensor.shape)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "print(targets)\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.min())\n",
    "plt.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683b812-1a18-4130-89b2-aa25e3b930e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"../labelme/MNIST_71/test_images\"\n",
    "# img = Image.open(str(data_dir)+\"/seven_170.png\")\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        # transforms.RandomAffine(20, translate=(0.20,0.20))\n",
    "    ])\n",
    "image_to_tensor = transform_test(img).unsqueeze(0)\n",
    "img_np = image_to_tensor.detach().cpu().squeeze().numpy()\n",
    "bass_seg = csv_mask_to_numpy(\"../labelme/MNIST_71/BASS_output/seven_170.csv\")\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, bass_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "748fd763-8212-482c-870c-6b65919fb91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fafe80e61f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        # transforms.RandomAffine(20, translate=(0.20,0.20))\n",
    "    ])\n",
    "images = Image.open(\"../labelme/MNIST_94/test_images/nine_102.png\")\n",
    "images = transform_test(images).unsqueeze(0)\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "036cbdfd-0c83-462d-81a8-300ef9117087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: target_layers is ignored in FullGrad. All bias layers will be used instead\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "cam = FullGrad(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.min())\n",
    "plt.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79a2508-c955-4c57-8d5e-2be0f67c98aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26461/2806886612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                      start_label=1)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m working_example = region_explainability(image = images, top_n_start = 1, \n\u001b[0m\u001b[1;32m     18\u001b[0m                                         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMU_class_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                         threshold = 0.8, top_n_stop = 21)\n",
      "\u001b[0;32m/tmp/ipykernel_26461/2997645144.py\u001b[0m in \u001b[0;36mregion_explainability\u001b[0;34m(image, top_n_start, model, SMU_class_index, threshold, top_n_stop)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;31m# sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# sm_idx1 = np.argmax(sm1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0msm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobfuscated_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0msm_idx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/adv_robustness/region_explainability/mnist_training/trainer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m                 \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_grad_cam/activations_and_gradients.py\u001b[0m in \u001b[0;36msave_activation\u001b[0;34m(self, module, input, output)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Just give an images and it will output\n",
    "\n",
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "\n",
    "working_example = region_explainability(image = images, top_n_start = 1, \n",
    "                                        model = model, SMU_class_index = 7, \n",
    "                                        threshold = 0.8, top_n_stop = 21)\n",
    "print(\"regions analyzed\", working_example[-2])\n",
    "sm1 = softmax(model(images.to(device)).cpu().detach().numpy()).squeeze()\n",
    "sm_idx1 = np.argmax(sm1)\n",
    "sm2 = softmax(model(working_example[0].to(device)).cpu().detach().numpy()).squeeze()\n",
    "sm_idx2 = np.argmax(sm2)\n",
    "print(\"Original Version Predicted Class:\",sm_idx1, \"   With Confidence:\", sm1[sm_idx1])\n",
    "print(\"Modified Version Predicted Class:\",sm_idx2, \"   With Confidence:\", sm2[sm_idx2])\n",
    "#print(sm2)\n",
    "\n",
    "#print(model(images.to(device)))\n",
    "#print(model(working_example[0].to(device)))\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "# ax1.imshow(images.detach().cpu().squeeze(), cmap='gray' )\n",
    "# plt.axis('off')\n",
    "# ax2.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)\n",
    "# plt.axis('off')\n",
    "# ax3.imshow(segmentation.mark_boundaries(img_np, segments_slic))\n",
    "# plt.axis('off')\n",
    "# ax4.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')\n",
    "# plt.axis('off')\n",
    "\n",
    "plot_images = (input_tensor.detach().cpu().squeeze(),\n",
    "                  grayscale_cam,\n",
    "                  segmentation.mark_boundaries(img_np, segments_slic),\n",
    "                  working_example[0].detach().cpu().squeeze())\n",
    "    \n",
    "figure_name = plt.figure(figsize=(14, 14))\n",
    "for i, img in enumerate(plot_images):\n",
    "    plt.subplot(1, 4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.margins(x=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "958bc42b-c1dd-4734-866b-800ff656ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.0, 36, 784], [2, 0.0068326807, 36, 784], [3, 0.048686378, 42, 784], [4, 0.00023984679, 36, 784], [5, 0.0, 24, 784], [6, 0.0037500043, 36, 784], [7, 0.08605875, 28, 784], [8, 0.010545408, 22, 784], [9, 0.0, 30, 784], [10, 0.14752428, 24, 784], [11, 0.37659183, 32, 784], [12, 0.041793425, 31, 784], [13, 0.044996284, 35, 784], [14, 0.23639867, 39, 784], [15, 0.0, 35, 784], [16, 1.1896104e-05, 30, 784], [17, 0.1213041, 43, 784], [18, 0.011130908, 20, 784], [19, 0.0, 28, 784], [20, 0.022265106, 35, 784], [21, 0.0, 33, 784], [22, 0.0, 25, 784], [23, 0.0, 30, 784], [24, 0.0, 29, 784], [25, 0.0, 25, 784]]\n"
     ]
    }
   ],
   "source": [
    "print(working_example[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f886cda-92c8-481a-82f5-e09414e6c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7  7  3  3  3  3  3  8  8  8  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7  7  7  3 10 10 10 10 10  8  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7  7 10 10 10 10 10 10 10  8  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7 10 10 10 10 10 10 10 10 10  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7 11 11 11 10 12 12 12 12 10 10 13  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7 11 11 11 11 12 12 12 12 12 12 12 13 13 13  8  8  9\n",
      "   9  9  9  9]\n",
      " [14 14 14 14 14 14 11 11 11 11 12 12 12 12 12 12 12 12 13 13 13 13 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 11 11 11 12 12 12 12 12 12 12 12 13 13 13 13 15 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 11 11 11 11 12 12 12 12 13 13 13 13 13 13 13 15 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 14 11 11 11 11 11 11 11 13 13 13 13 13 13 13 15 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 14 11 11 11 11 11 11 11 13 13 13 13 13 13 16 16 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 14 17 17 17 17 17 17 17 17 13 18 13 13 16 16 16 16 15\n",
      "  15 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 18 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 21 21 21 17 17 17 22 22 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 18 18 18 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]]\n"
     ]
    }
   ],
   "source": [
    "print(segments_slic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964de447-2c49-4819-9783-95943389ecac",
   "metadata": {},
   "source": [
    "# Quantitative Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30cc76-24d6-4a87-808a-12501799d81a",
   "metadata": {},
   "source": [
    "## Experimenting with different clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c737dd-66be-48ea-8bfc-2f3ff778abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            if sm_idx1 != sm_idx2:\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ea514-1555-436c-aa39-76858a9cf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "i = 0\n",
    "n = 0\n",
    "image_info_list = []\n",
    "while i < 50:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    #n += 1\n",
    "    #print()\n",
    "    #print(predicted)\n",
    "    \n",
    "    logits = model(images.to(device)).cpu()\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    # Change probs[int] to int = SMU class index\n",
    "    probs_orig = probs[0]\n",
    "    #print(probs)\n",
    "    \n",
    "    \n",
    "    if just_label == 7 and predicted ==7:\n",
    "        print('index:', i+1)\n",
    "        i += 1\n",
    "        \n",
    "        re = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 7)\n",
    "        \n",
    "        \n",
    "        image_info = []\n",
    "        example = re[0]\n",
    "        exam_img = good_img_transform(example)\n",
    "        logits = model(exam_img).cpu()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = probs.detach().cpu()\n",
    "        probs = probs.tolist()[0]\n",
    "        # Change probs[int] to int = SMU class index\n",
    "        probs_obf = probs[0]\n",
    "        #print(probs)\n",
    "        image_info.append(probs_orig)\n",
    "        image_info.append(probs_obf)\n",
    "        \n",
    "        num_pixels_obf = re[1]\n",
    "        image_info.append(num_pixels_obf)\n",
    "        image_info.append(re[5])\n",
    "        sm1 = softmax(model(example.to(device)).cpu().detach().numpy()).squeeze()\n",
    "        sm_idx1 = np.argmax(sm1)\n",
    "        #sm_idx1 is the predicted class of the obfuscated image\n",
    "        image_info.append(sm_idx1)\n",
    "        \n",
    "        image_info_list.append(image_info)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b12456-0462-4376-ba6c-ce7a3fd69479",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "total = len(image_info_list)\n",
    "total_pix = total * 28 * 28\n",
    "total_obf = 0\n",
    "total_orig_conf = 0\n",
    "total_obf_conf = 0\n",
    "total_regions = 0\n",
    "preds_on_images = []\n",
    "for i in range(len(image_info_list)):\n",
    "    total_orig_conf += image_info_list[i][0]\n",
    "    total_obf_conf += image_info_list[i][1]\n",
    "    total_obf += image_info_list[i][2]\n",
    "    total_regions += image_info_list[i][3]\n",
    "    preds_on_images.append(image_info_list[i][4])\n",
    "\n",
    "\n",
    "array_np = np.array(preds_on_images)\n",
    "unique, counts = np.unique(array_np, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "\n",
    "print(\"Experiment with SLIC with 25 segments and 1 compactness\")\n",
    "print(\"Average number of regions analyzed: \", total_regions / total)\n",
    "print(\"Average confidence change: \", ((total_orig_conf - total_obf_conf) / total) )\n",
    "print(\"Distribution of changed to class: \", dict(zip(unique, counts)))\n",
    "print(\"Average obfuscation: \",total_obf / total_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf22ea-15a9-4b6d-96b4-a2633eec562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with felz with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "\n",
    "    while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "                                  num_obf_pixels = num_pixels_changed,\n",
    "                                  model = model,\n",
    "                                  image = obfuscated_image,\n",
    "                                  SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            if score < 0.5:\n",
    "                prob = score\n",
    "                image_versions.append(obfuscated_image)\n",
    "                scores.append(score)\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "                \n",
    "                #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    \n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db9528-ab90-4a3c-b634-f49274165c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
