{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d7b10c3-cc5f-4f28-86dd-2b075ccf4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models as tvmodels\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision.models as torchvisionmodels\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import LayerGradCam\n",
    "from captum.attr import visualization\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from dask_image.imread import imread\n",
    "from dask_image import ndfilters, ndmorph, ndmeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_image import ndmeasure\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28e9f04-b41c-4497-b72d-72955325ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trainer import *\n",
    "\n",
    "allowed_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "labels_map = {\n",
    "    0: '0',\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6',\n",
    "    7: '7',\n",
    "    8: '8',\n",
    "    9: '9'\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# model = MNIST_model(len(allowed_classes))\n",
    "# checkpoint = torch.load('resnet_models/resnet18.pt')\n",
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "model_dict = torch.load('resnet_models/grad_cam_model.pt')\n",
    "model = gradcam_model()\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data, valid_data, test_data = create_dataloaders_MNIST(batch_size)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model and data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfc9b65b-403c-4762-a4a3-d68efd8d0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_img_transform = transforms.Normalize((0.1307,), (0.3081,))\n",
    "# This is to reverse the normalization done to the images that centered them around imagenet mean and std\n",
    "# The invTrans should be used on images before saving them.\n",
    "invTrans = transforms.Normalize((1/0.1307,), (1/0.3081,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0334fe86-7945-4b86-ad21-9599388b54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f374c55a310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 0, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 5, 8, 9, 13, 16, 17, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dcbd51-01e7-4670-9290-0a6d4f80e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "from pytorch_grad_cam import XGradCAM, GradCAM, FullGrad, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from skimage.segmentation import slic, felzenszwalb, quickshift, watershed\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43085c17-7287-4bd2-9a32-43fb2384b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayscale_grad_cam(image, SMU_class_index):\n",
    "    input_tensor = image.to(device)\n",
    "    targets = [ClassifierOutputTarget(SMU_class_index)]\n",
    "    #target_layers = [model.layer4[-1]]\n",
    "    target_layers = [model.layer2]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    return(grayscale_cam)\n",
    "\n",
    "def segmentation_info(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "# I want to get the average attribution score for each segment\n",
    "def cam_processor_for_segments(grayscale_cam_output, segments_slic):\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    region_attr_score = []\n",
    "    final_region_attr_score = []\n",
    "    num_pixels_in_region_list = []\n",
    "    \n",
    "    for i in (list_unique_regions):\n",
    "        row_counter = 0\n",
    "        column_counter = 0\n",
    "        region_attr_score = []\n",
    "        num_pixels_in_region = 0\n",
    "        for row in grayscale_cam_output:\n",
    "            for cell in row:\n",
    "                current_score = grayscale_cam_output[row_counter, column_counter]\n",
    "                current_region = segments_slic[row_counter, column_counter]\n",
    "                if current_region == i:\n",
    "                    region_attr_score.append(current_score)\n",
    "                    num_pixels_in_region += 1\n",
    "                column_counter +=1\n",
    "            row_counter += 1\n",
    "            column_counter = 0\n",
    "        avg_score = np.mean(region_attr_score)\n",
    "        final_region_attr_score.append(avg_score)\n",
    "        num_pixels_in_region_list.append(num_pixels_in_region)\n",
    "    \n",
    "    unique_region_info = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(final_region_attr_score[i-1])\n",
    "        image_list.append(num_pixels_in_region_list[i-1])\n",
    "        image_list.append(np.sum(num_pixels_in_region_list))\n",
    "        unique_region_info.append(image_list)\n",
    "    \n",
    "    return(unique_region_info)\n",
    "\n",
    "\n",
    "def get_feature_masks(image, attributions, segments_slic):\n",
    "    segments_slic_1 = segments_slic\n",
    "    features = []\n",
    "    for i in attributions:\n",
    "        feature = np.where(i==segments_slic_1, 1, 0)\n",
    "        features.append(feature)\n",
    "        \n",
    "    return(features)\n",
    "\n",
    "\n",
    "def attribution_ranker(cam_processor_for_segments_output, num_top_attr):\n",
    "    ranked_images = sorted(cam_processor_for_segments_output, key=itemgetter(1), reverse=True)\n",
    "    top_ranked_features = []\n",
    "    for i in range(num_top_attr):\n",
    "        top_ranked_features.append(ranked_images[i][0])\n",
    "        \n",
    "    return top_ranked_features\n",
    "\n",
    "\n",
    "\n",
    "def image_rankings(get_image_versions):\n",
    "    #for idx in iterative_Grad_CAM_counterfactual_masking_output\n",
    "    ranked_images = sorted(get_image_versions, key=itemgetter(3))\n",
    "    \n",
    "    return ranked_images\n",
    "\n",
    "def blur_image_from_attribution(image, attribution_map):\n",
    "    # attribution map is the attributions after being passed through the attribution processor\n",
    "    # image is a tensor\n",
    "    # will output the blurred image based on the attribution map\n",
    "    \n",
    "    \n",
    "    #average_img = image.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    #avg = np.average(average_img)\n",
    "    #blurred_img = cv2.GaussianBlur(image.squeeze().cpu().permute(1, 2, 0).numpy(), (181, 181), 0)\n",
    "    avg = np.float32(-0.4242)\n",
    "    #avg_img = np.where(average_img > 9999, average_img, avg)\n",
    "    \n",
    "    #attribution_map = attribution_map.detach().squeeze().cpu().numpy()\n",
    "    \n",
    "    mask = [attribution_map]\n",
    "    mask = np.array(mask).squeeze()\n",
    "    #mask = mask.transpose(1,2,0)\n",
    "    #print(mask.shape)\n",
    "    #print(image.squeeze().cpu().numpy().shape)\n",
    "    \n",
    "    \n",
    "    out = np.where(mask==np.array([0]), image.squeeze().cpu().numpy(), avg)\n",
    "    #out = np.where(mask==np.array([0, 0, 0]), image.squeeze().cpu().permute(1, 2, 0).numpy(), blurred_img)\n",
    "    \n",
    "    out = torch.tensor(out)\n",
    "    out = out\n",
    "    out = out.unsqueeze(0)\n",
    "    out = out.unsqueeze(0)\n",
    "    #print(out.shape)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5bfe8a8e-6985-4874-beae-7986815658f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_info_slic(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "def segmentation_info_felzenszwalb(image, scale, sigma, min_size):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_felz = felzenszwalb(img_np, scale=scale, sigma=sigma, min_size=min_size)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "\n",
    "def softmax_score(num_total_pixels, num_obf_pixels, model, image, SMU_class_index):\n",
    "    #image = good_img_transform(image)\n",
    "    image = image\n",
    "    logits = model(image).cpu()\n",
    "    #print(logits)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    probs = probs[SMU_class_index]\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index, threshold, top_n_stop):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    best_score_other_class = 0\n",
    "    \n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        print(features_list[0])\n",
    "        \n",
    "        previous_powerset_list = list(more_itertools.powerset(features_1[0:top_n-1]))\n",
    "        previous_powerset_list = [list(ele) for ele in previous_powerset_list if any(ele[0]) == True]\n",
    "        \n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        #print(powerset_list)\n",
    "        powerset_list = previous_powerset_list + [list(ele) for ele in powerset_list if any(ele[0]) == True]\n",
    "        print(powerset_list)\n",
    "        powerset_list = np.unique(powerset_list)\n",
    "        \n",
    "        print(powerset_list)\n",
    "        #[ [[]], [[]] ];\n",
    "        \n",
    "        \n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            \n",
    "            if (sm_idx1 != sm_idx2) and sm2[sm_idx2] > best_score_other_class:\n",
    "                best_score_other_class = sm2[sm_idx2] \n",
    "            \n",
    "            if (sm_idx1 != sm_idx2) and (sm2[sm_idx2] > threshold):\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "        if top_n == top_n_stop:\n",
    "            return -1\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa1a54de-3080-4618-bef6-62059c120e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd6bb784f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 26, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 0, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e68775b9-cda0-47e9-ae96-cb31617d6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "index: 17\n",
      "index: 26\n",
      "index: 34\n",
      "index: 36\n",
      "index: 41\n",
      "index: 60\n",
      "index: 64\n",
      "index: 70\n",
      "index: 75\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "n = 0\n",
    "while i < 10:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    \n",
    "    if just_label == 7 and predicted == 7:\n",
    "        print('index:', n)\n",
    "        i += 1\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b73904f2-f297-44c2-b772-95921e51ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb8f211db20>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ec91c4a-f5a8-47df-880e-fa8ef3e2f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_example = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "079ce5a4-26e3-4d45-ac78-d2b727281400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[[1, 0.22351724, 36, 784], [2, 0.30417144, 33, 784], [3, 0.38000596, 30, 784], [4, 0.5808148, 28, 784], [5, 0.6820013, 34, 784], [6, 0.5424326, 28, 784], [7, 0.13844198, 36, 784], [8, 0.32641846, 40, 784], [9, 0.5239489, 47, 784], [10, 0.62660193, 25, 784], [11, 0.7088958, 20, 784], [12, 0.10682348, 36, 784], [13, 0.40781006, 34, 784], [14, 0.67056465, 37, 784], [15, 0.7335447, 35, 784], [16, 0.47609615, 36, 784], [17, 0.7418132, 20, 784], [18, 0.7551767, 23, 784], [19, 0.10629138, 34, 784], [20, 0.8421018, 25, 784], [21, 0.1543859, 34, 784], [22, 0.044165593, 33, 784], [23, 0.01716993, 30, 784], [24, 0.20089908, 25, 784], [25, 0.24637978, 25, 784]]\n",
      "tensor([[-0.7905, -1.4968, -0.5957, -1.3958, -0.8397,  0.1229, -2.2938, 10.1160,\n",
      "         -1.1916, -0.8480]], device='cuda:0',\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[-4.5783,  1.2394, -0.2601, -1.5987, -2.2005, -0.0164, -3.4140,  0.5213,\n",
      "         -2.1525, -2.5007]], device='cuda:0',\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6736673cd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(working_example[-2])\n",
    "print(working_example[-1])\n",
    "print(model(images.to(device)))\n",
    "print(model(working_example[0].to(device)))\n",
    "plt.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "39d1964c-c910-479b-89f9-dc56a3a40c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd5a336c70>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 34, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens:index: 0, 17, 26, 34, 36, 41, 60, 64, 70, 75\n",
    "# Good outputs 17, 48, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc66067e-f097-45ba-9716-46abc5f87595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4bca473ca0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.min())\n",
    "plt.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e79a2508-c955-4c57-8d5e-2be0f67c98aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9273/1620825729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                      start_label=1)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mworking_example\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion_explainability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMU_class_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regions analyzed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9273/2995499807.py\u001b[0m in \u001b[0;36mregion_explainability\u001b[0;34m(image, top_n_start, model, SMU_class_index, threshold, top_n_stop)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprevious_powerset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_itertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpowerset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mprevious_powerset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprevious_powerset_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9273/2995499807.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mprevious_powerset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_itertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpowerset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mprevious_powerset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprevious_powerset_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mele\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Just give an images and it will output\n",
    "\n",
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "\n",
    "working_example = region_explainability(image = images, top_n_start = 2, model = model, SMU_class_index = 7, threshold = 0.8, top_n_stop = 20)\n",
    "print(\"regions analyzed\", working_example[-2])\n",
    "sm1 = softmax(model(images.to(device)).cpu().detach().numpy()).squeeze()\n",
    "sm_idx1 = np.argmax(sm1)\n",
    "sm2 = softmax(model(working_example[0].to(device)).cpu().detach().numpy()).squeeze()\n",
    "sm_idx2 = np.argmax(sm2)\n",
    "print(\"Original Version Predicted Class:\",sm_idx1, \"   With Confidence:\", sm1[sm_idx1])\n",
    "print(\"Modified Version Predicted Class:\",sm_idx2, \"   With Confidence:\", sm2[sm_idx2])\n",
    "#print(sm2)\n",
    "\n",
    "#print(model(images.to(device)))\n",
    "#print(model(working_example[0].to(device)))\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "ax1.imshow(images.detach().cpu().squeeze(), cmap='gray' )\n",
    "ax2.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)\n",
    "ax3.imshow(segmentation.mark_boundaries(img_np, segments_slic))\n",
    "ax4.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb13fc79-9f9d-4173-94c2-aebfe5ec8e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0.005982419, 36, 784], [2, 0.0026869853, 36, 784], [3, 0.07836885, 36, 784], [4, 0.3689115, 34, 784], [5, 0.47296873, 31, 784], [6, 0.009011468, 36, 784], [7, 0.026845593, 40, 784], [8, 0.14010577, 31, 784], [9, 0.3462319, 37, 784], [10, 0.3684675, 28, 784], [11, 0.69863135, 24, 784], [12, 0.009606956, 36, 784], [13, 0.11113425, 34, 784], [14, 0.44087908, 39, 784], [15, 0.6812516, 26, 784], [16, 0.13962221, 37, 784], [17, 0.65417826, 28, 784], [18, 0.016456205, 35, 784], [19, 0.16832516, 30, 784], [20, 0.3716643, 21, 784], [21, 0.018251318, 32, 784], [22, 0.019858956, 30, 784], [23, 0.018787958, 18, 784], [24, 0.021452442, 24, 784], [25, 0.021324772, 25, 784]]\n",
      "tensor([[-2.8257,  2.8145]], device='cuda:0',\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[ 2.5814, -2.5924]], device='cuda:0',\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#print(segments_slic)\n",
    "print(working_example[-1])\n",
    "\n",
    "# image = images\n",
    "# logits = model(images).cpu()\n",
    "# probs = F.softmax(logits, dim=1)\n",
    "# probs = probs.detach().cpu()\n",
    "# probs = probs.tolist()[0]\n",
    "# probs = probs[1]\n",
    "# print(probs)\n",
    "\n",
    "\n",
    "print(model(images.to(device)))\n",
    "print(model(working_example[0].to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f886cda-92c8-481a-82f5-e09414e6c08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "964de447-2c49-4819-9783-95943389ecac",
   "metadata": {},
   "source": [
    "# Quantitative Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30cc76-24d6-4a87-808a-12501799d81a",
   "metadata": {},
   "source": [
    "## Experimenting with different clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9c737dd-66be-48ea-8bfc-2f3ff778abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            if sm_idx1 != sm_idx2:\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ea514-1555-436c-aa39-76858a9cf280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 1\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "index: 2\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "index: 3\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n",
      "Regions analyzed 12\n",
      "Regions analyzed 13\n",
      "Regions analyzed 14\n",
      "Regions analyzed 15\n",
      "Regions analyzed 16\n",
      "Regions analyzed 17\n",
      "Regions analyzed 18\n",
      "index: 4\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "index: 5\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "index: 6\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "index: 7\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n",
      "Regions analyzed 12\n",
      "Regions analyzed 13\n",
      "Regions analyzed 14\n",
      "Regions analyzed 15\n",
      "Regions analyzed 16\n",
      "Regions analyzed 17\n",
      "Regions analyzed 18\n",
      "Regions analyzed 19\n",
      "index: 8\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "index: 9\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "index: 10\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "index: 11\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n",
      "index: 12\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n",
      "Regions analyzed 12\n",
      "Regions analyzed 13\n",
      "Regions analyzed 14\n",
      "Regions analyzed 15\n",
      "index: 13\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n",
      "Regions analyzed 8\n",
      "Regions analyzed 9\n",
      "Regions analyzed 10\n",
      "Regions analyzed 11\n",
      "Regions analyzed 12\n",
      "Regions analyzed 13\n",
      "Regions analyzed 14\n",
      "Regions analyzed 15\n",
      "Regions analyzed 16\n",
      "Regions analyzed 17\n",
      "Regions analyzed 18\n",
      "Regions analyzed 19\n"
     ]
    }
   ],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "i = 0\n",
    "n = 0\n",
    "image_info_list = []\n",
    "while i < 50:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    #n += 1\n",
    "    #print()\n",
    "    #print(predicted)\n",
    "    \n",
    "    logits = model(images.to(device)).cpu()\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    # Change probs[int] to int = SMU class index\n",
    "    probs_orig = probs[0]\n",
    "    #print(probs)\n",
    "    \n",
    "    \n",
    "    if just_label == 7 and predicted ==7:\n",
    "        print('index:', i+1)\n",
    "        i += 1\n",
    "        \n",
    "        re = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 7)\n",
    "        \n",
    "        \n",
    "        image_info = []\n",
    "        example = re[0]\n",
    "        exam_img = good_img_transform(example)\n",
    "        logits = model(exam_img).cpu()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = probs.detach().cpu()\n",
    "        probs = probs.tolist()[0]\n",
    "        # Change probs[int] to int = SMU class index\n",
    "        probs_obf = probs[0]\n",
    "        #print(probs)\n",
    "        image_info.append(probs_orig)\n",
    "        image_info.append(probs_obf)\n",
    "        \n",
    "        num_pixels_obf = re[1]\n",
    "        image_info.append(num_pixels_obf)\n",
    "        image_info.append(re[5])\n",
    "        sm1 = softmax(model(example.to(device)).cpu().detach().numpy()).squeeze()\n",
    "        sm_idx1 = np.argmax(sm1)\n",
    "        #sm_idx1 is the predicted class of the obfuscated image\n",
    "        image_info.append(sm_idx1)\n",
    "        \n",
    "        image_info_list.append(image_info)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b12456-0462-4376-ba6c-ce7a3fd69479",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "total = len(image_info_list)\n",
    "total_pix = total * 28 * 28\n",
    "total_obf = 0\n",
    "total_orig_conf = 0\n",
    "total_obf_conf = 0\n",
    "total_regions = 0\n",
    "preds_on_images = []\n",
    "for i in range(len(image_info_list)):\n",
    "    total_orig_conf += image_info_list[i][0]\n",
    "    total_obf_conf += image_info_list[i][1]\n",
    "    total_obf += image_info_list[i][2]\n",
    "    total_regions += image_info_list[i][3]\n",
    "    preds_on_images.append(image_info_list[i][4])\n",
    "\n",
    "\n",
    "array_np = np.array(preds_on_images)\n",
    "unique, counts = np.unique(array_np, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "\n",
    "print(\"Experiment with SLIC with 100 segments and 50 compactness\")\n",
    "print(\"Average number of regions analyzed: \", total_regions / total\n",
    "print(\"Average confidence change: \", ((total_orig_conf - total_obf_conf) / total) )\n",
    "print(\"Distribution of changed to class: \", dict(zip(unique, counts)))\n",
    "print(\"Average obfuscation: \",total_obf / total_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf22ea-15a9-4b6d-96b4-a2633eec562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with felz with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "\n",
    "    while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "                                  num_obf_pixels = num_pixels_changed,\n",
    "                                  model = model,\n",
    "                                  image = obfuscated_image,\n",
    "                                  SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            if score < 0.5:\n",
    "                prob = score\n",
    "                image_versions.append(obfuscated_image)\n",
    "                scores.append(score)\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "                \n",
    "                #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    \n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
