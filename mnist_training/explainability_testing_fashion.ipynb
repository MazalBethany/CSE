{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d7b10c3-cc5f-4f28-86dd-2b075ccf4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models as tvmodels\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import torchvision.models as torchvisionmodels\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import itertools\n",
    "import more_itertools\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import LayerGradCam\n",
    "from captum.attr import visualization\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from dask_image.imread import imread\n",
    "from dask_image import ndfilters, ndmorph, ndmeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from dask_image import ndmeasure\n",
    "\n",
    "from operator import itemgetter\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d28e9f04-b41c-4497-b72d-72955325ac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and data loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trainer import *\n",
    "\n",
    "allowed_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "labels_map = {\n",
    "    0: '0',\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6',\n",
    "    7: '7',\n",
    "    8: '8',\n",
    "    9: '9'\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# model = MNIST_model(len(allowed_classes))\n",
    "# checkpoint = torch.load('resnet_models/resnet18.pt')\n",
    "# model.load_state_dict(checkpoint)\n",
    "\n",
    "model_dict = torch.load('resnet_models/grad_cam_model_fashion.pt')\n",
    "model = gradcam_model()\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data, valid_data, test_data = create_dataloaders_MNIST_fashion(batch_size)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Model and data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc9b65b-403c-4762-a4a3-d68efd8d0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_img_transform = transforms.Normalize((0.5,), (0.5,))\n",
    "# This is to reverse the normalization done to the images that centered them around imagenet mean and std\n",
    "# The invTrans should be used on images before saving them.\n",
    "invTrans = transforms.Normalize((1/0.5,), (1/0.5,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0334fe86-7945-4b86-ad21-9599388b54d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n",
      "tensor([9], device='cuda:0')\n",
      "tensor(1.)\n",
      "tensor([[[[-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4882, -1.4961,\n",
      "           -1.5000, -1.5000, -1.4725, -1.5000, -1.3549, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4961,\n",
      "           -1.4922, -1.5000, -1.3941, -1.1706, -1.4569, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.0333, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4961,\n",
      "           -1.5000, -1.5000, -1.1549, -0.9392, -1.0686, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.4137, -1.1353, -1.0843, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4843,\n",
      "           -1.5000, -1.2922, -0.9941, -1.0294, -0.9235, -0.8137, -0.8843,\n",
      "           -0.8490, -0.9706, -0.8961, -0.8412, -0.9510, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4922, -1.5000,\n",
      "           -1.4569, -0.9627, -0.9902, -0.9980, -0.8725, -0.8098, -0.8765,\n",
      "           -0.8451, -0.8020, -0.9157, -0.9078, -0.9353, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4961,\n",
      "           -1.5000, -1.4922, -1.4961, -1.5000, -1.4882, -1.5000, -1.5000,\n",
      "           -1.0490, -1.0529, -1.0843, -0.9627, -0.8412, -0.9000, -0.8882,\n",
      "           -0.8529, -0.8451, -0.9392, -0.8843, -0.8804, -1.4569, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.4961, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.4882, -1.5000, -1.5000, -1.1510,\n",
      "           -0.9549, -1.1471, -1.1314, -0.9000, -0.9157, -0.9863, -0.9078,\n",
      "           -0.8373, -0.8255, -0.9392, -0.8765, -0.8373, -1.3118, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.4922,\n",
      "           -1.4843, -1.4961, -1.5000, -1.5000, -1.5000, -1.1157, -0.9667,\n",
      "           -1.0686, -1.0725, -1.0686, -0.8647, -0.9706, -0.9353, -0.9157,\n",
      "           -0.8765, -0.8451, -0.9353, -0.8804, -0.8373, -1.0333, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.4922, -1.4922, -1.4961, -1.4922, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.3980, -1.0765, -1.0412, -1.1118,\n",
      "           -1.0647, -1.0412, -0.9667, -0.8882, -0.9745, -0.8961, -0.8961,\n",
      "           -0.8882, -0.8725, -0.9471, -0.9235, -0.8882, -0.8020, -1.5000],\n",
      "          [-1.4882, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.4176, -1.2922, -1.1392, -1.0412, -1.0647, -1.0961, -1.0490,\n",
      "           -0.9941, -0.9745, -0.9392, -0.8961, -0.8529, -0.8333, -0.8961,\n",
      "           -0.9078, -0.8961, -0.9392, -0.9588, -0.9118, -0.8529, -1.3314],\n",
      "          [-1.5000, -1.5000, -1.4098, -1.2882, -1.2451, -1.2020, -1.1667,\n",
      "           -1.0373, -0.9980, -1.0176, -1.0647, -1.0569, -1.0373, -1.0020,\n",
      "           -1.0098, -0.9549, -0.9784, -0.9667, -0.8725, -0.9510, -0.8922,\n",
      "           -0.8686, -0.9353, -0.8922, -0.8255, -0.8686, -0.7588, -1.2569],\n",
      "          [-1.5000, -1.2333, -1.1314, -1.1471, -1.0647, -1.0529, -1.0647,\n",
      "           -1.0529, -1.0490, -1.0020, -0.9706, -0.9667, -0.9392, -1.0059,\n",
      "           -1.0020, -0.9078, -0.8961, -0.9392, -0.9196, -1.0098, -0.8647,\n",
      "           -0.8647, -0.9353, -0.9588, -0.9000, -0.8647, -0.7314, -1.2725],\n",
      "          [-1.2255, -0.8373, -0.9941, -1.0922, -1.1157, -1.1078, -1.1314,\n",
      "           -1.1196, -1.1157, -1.1000, -1.0765, -1.0843, -1.0333, -1.0294,\n",
      "           -0.9941, -0.9157, -0.8882, -0.8451, -0.7549, -0.7549, -0.7314,\n",
      "           -0.7235, -0.7235, -0.7667, -0.7275, -0.7588, -0.7784, -1.3588],\n",
      "          [-1.4373, -1.0059, -0.8294, -0.7627, -0.7627, -0.7784, -0.8294,\n",
      "           -0.9000, -0.9706, -1.0294, -1.0059, -1.0020, -0.9275, -0.7745,\n",
      "           -0.7353, -0.6804, -0.6843, -0.5000, -0.6804, -0.8059, -0.5392,\n",
      "           -0.5118, -0.5157, -0.5157, -0.5314, -0.6373, -0.6922, -1.3078],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.4529, -1.2373, -1.0843, -0.8569,\n",
      "           -0.7745, -0.7196, -0.6765, -0.6725, -0.6765, -0.6843, -0.7549,\n",
      "           -0.9118, -1.1784, -1.4686, -1.5000, -1.5000, -1.5000, -0.8020,\n",
      "           -0.6843, -0.7627, -0.8137, -0.8647, -0.8804, -0.9078, -1.4569],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000],\n",
      "          [-1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000,\n",
      "           -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000, -1.5000]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f34b843f3d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPUklEQVR4nO3df6yW5X3H8c9HVFQURRAEqkIromVGuxBR0cWltjj/0Wpsyh+LcyTUpC41mdlM90dNliW6rVviP01oasqWzqaJkpJmrGWmqds/VSQM8UcLNhA54UcQFERQge/+ODfLUc99Xcfnx3ke932/kpPznPt77ue5uOHD/Tz3dV/X5YgQgP//zhh0AwBMDsIOJEHYgSQIO5AEYQeSOHMyX8w2l/6BPosIj7e9qzO77Tts/9b2DtuPdvNcAPrLnfaz254i6XeSviJpt6QXJa2MiFcL+3BmB/qsH2f2GyTtiIjfR8QHkn4i6a4ung9AH3UT9vmS3hzz8+5m20fYXm17k+1NXbwWgC71/QJdRKyRtEbibTwwSN2c2UckXTbm58812wAMoW7C/qKkRbYX2j5b0jckre9NswD0Wsdv4yPihO2HJP1C0hRJT0XEKz1rGYCe6rjrraMX4zM70Hd9uakGwGcHYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJjtdnlyTbOyUdkXRS0omIWNqLRgHova7C3vjjiDjQg+cB0Ee8jQeS6DbsIemXtl+yvXq8X7C92vYm25u6fC0AXXBEdL6zPT8iRmzPlrRR0l9ExPOF3+/8xQBMSER4vO1dndkjYqT5vl/SOkk3dPN8APqn47Dbnmb7gtOPJX1V0rZeNQxAb3VzNX6OpHW2Tz/Pv0XEf/SkVQB6rqvP7J/6xfjMDvRdXz6zA/jsIOxAEoQdSIKwA0kQdiCJXgyEAQZiypQpxfqpU6daa932Qk2dOrVYf//994v1K6+8srW2Y8eOjtpUw5kdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Kgnz25Zohyx/VSX7YkzZ8/v7V20003FffdsGFDsX706NFivZ9q/eg19957b2vtiSee6Oq523BmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk6GdHUa0fvebWW29trS1btqy477x584r1J598sqM29cLs2bOL9RUrVhTrhw8f7mVzJoQzO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQT97crW510+cOFGsL126tFi/5pprWmv79u0r7rto0aJifd26dcX6wYMHW2vnnntucd9du3YV6zNnzizWp0+fXqzv3r27WO+H6pnd9lO299veNmbbxbY32t7efJ/R32YC6NZE3sb/SNIdH9v2qKTnImKRpOeanwEMsWrYI+J5SR9/P3SXpLXN47WS7u5tswD0Wqef2edExJ7m8V5Jc9p+0fZqSas7fB0APdL1BbqICNutq+RFxBpJaySp9HsA+qvTrrd9tudKUvN9f++aBKAfOg37ekn3N4/vl/Sz3jQHQL9U38bbflrSbZJm2d4t6buSHpf0U9urJO2S9PV+NhKdO+OM8v/ntX70adOmFev33XdfsV6aX/2cc84p7nvBBRcU67U57Ut/9tq+S5YsKdbffPPNYv3QoUPF+plnTv4tLtVXjIiVLaUv97gtAPqI22WBJAg7kARhB5Ig7EAShB1IgiGuE1Tqqoko3xhY6/6q7V+rl4apnjx5srhvzYMPPlis7927t1g/fvx4a23BggXFfWtdc7UhsqXjUpsiu7Yc9AcffFCs14a4Tp06tbVW6+7sdKlqzuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESafvbakMZu+7pLul32uDbdczd96StXtg1qHHXppZcW65s3by7WzzrrrNbaRRddVNz3rbfeKtZLU0VL0qxZs1prteGztWNeU7u34rzzzmut1abQ3rJlSydN4swOZEHYgSQIO5AEYQeSIOxAEoQdSIKwA0mk6Wfvpp9cKveb1vpUa/3gtbZ104/+wAMPFOuLFy8u1mtTJpf6sqXy/Q21ZZNHRkaK9Vpfeen+hvfee6+4b20sfbf3bZSsWLGiWKefHUARYQeSIOxAEoQdSIKwA0kQdiAJwg4k8ZnqZ6/1Z5fU+j1r/aalPttux6vXzJs3r1i/5557Wmu1vuzt27cX6+eff36xXpr/XJJmzpzZWqvNvV77OyuNCa+p3btQWmp6IvvX5nYv/ZtZvnx5cd9OVdNj+ynb+21vG7PtMdsjtrc0X3f2pXUAemYip8ofSbpjnO3/HBHXN1//3ttmAei1atgj4nlJ5fl/AAy9bi7QPWR7a/M2f0bbL9lebXuT7U1dvBaALnUa9u9L+oKk6yXtkfS9tl+MiDURsTQilnb4WgB6oKOwR8S+iDgZEack/UDSDb1tFoBe6yjstueO+fFrkra1/S6A4VDtZ7f9tKTbJM2yvVvSdyXdZvt6SSFpp6RvTvQFu1lLvJ/92d2MP77kkkuK9SuuuKJYv/rqq4v1uXPnFuul/urDhw8X963N3V5bZ7w0L7xU7oev/X3Wjlvttd9+++3W2ocffljct9a22j0fx44dK9ZLOThy5Ehx3yVLlrTW3njjjdZaNewRMd4qAj+s7QdguHC7LJAEYQeSIOxAEoQdSIKwA0lM+hDXbqZFnjNnTmut1k0zbdq0ruqloaILFy4s7lsbilnrBnr33XeL9VI30IUXXljctzYE9sSJE8V67c9WmrK5Noz07LPPLtb37NlTrJf+7LV2Hzp0qFivDf2dMaP1DnJJ5SGwtWWyS8OGd+3a1VrjzA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSQzVVNK33357sV6aUrnWVz179uxivTZksTTksfbatSGLtT7bWr9raRrs2lTPtf7k2nGptb00lLM23XLtuL3zzjvFeu3vvBu141YbIlu6v6F2f0Hp3ofSUG3O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxKT2s0+fPl033nhja33VqlXF/V9//fXWWm1sc21K5VJ/sFSerrm2b02tP7nW71qaI6A2FXRtqeraePdaf3Jpuufa/QOl+Quk8pTKtdfu9u+sdo9Abbz88ePHO37u/fv3t9ZKffCc2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiUntZz969KheeOGF1nqpD16Srr322tba8uXLO26XVJ8fvdQXfvDgweK+tXptXHatn73UV16aY1ySFi9eXKzX+otr/fil8dXXXXddcd+tW7cW6zt37izWS/Mj1Mb5d7OEt1T/9zQyMtJaq90TUppDoDT/QPXMbvsy27+y/artV2x/u9l+se2Ntrc338uz4gMYqIm8jT8h6S8j4ouSbpT0LdtflPSopOciYpGk55qfAQypatgjYk9EbG4eH5H0mqT5ku6StLb5tbWS7u5TGwH0wKf6zG57gaQvSfqNpDkRcfqG9L2Sxr2R2fZqSaubxx03FEB3Jnw13vb5kp6R9HBEfOQKQoxezRj3ikZErImIpRGxtDZ5IYD+mVD6bJ+l0aD/OCKebTbvsz23qc+V1D4UB8DAudbF4NH33mslHYyIh8ds/wdJb0XE47YflXRxRPxV5bm6688oqE1pvGzZsmL9qquuKtZvvvnm1lptyuJa91Rtuejax5/S32FtCGqtW7A0rFiSNm7cWKxv2LChtVYa5tkL69evb61dfvnlxX0PHDhQrNeGJdfqpa652lLWjzzySGvt2LFjOnny5Lj/YCbymX25pD+V9LLtLc2270h6XNJPba+StEvS1yfwXAAGpBr2iPhvSW2nli/3tjkA+oUrZkAShB1IgrADSRB2IAnCDiRR7Wfv6Yv1sZ8dwKiIGLf3jDM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kUQ277cts/8r2q7Zfsf3tZvtjtkdsb2m+7ux/cwF0qrpIhO25kuZGxGbbF0h6SdLdGl2P/d2I+McJvxiLRAB917ZIxETWZ98jaU/z+Ijt1yTN723zAPTbp/rMbnuBpC9J+k2z6SHbW20/ZXtGyz6rbW+yvam7pgLoxoTXerN9vqRfS/q7iHjW9hxJBySFpL/V6Fv9P688B2/jgT5rexs/obDbPkvSzyX9IiL+aZz6Akk/j4g/qDwPYQf6rOOFHW1b0g8lvTY26M2Fu9O+Jmlbt40E0D8TuRp/i6T/kvSypFPN5u9IWinpeo2+jd8p6ZvNxbzSc3FmB/qsq7fxvULYgf5jfXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAS1Qkne+yApF1jfp7VbBtGw9q2YW2XRNs61cu2XdFWmNTx7J94cXtTRCwdWAMKhrVtw9ouibZ1arLaxtt4IAnCDiQx6LCvGfDrlwxr24a1XRJt69SktG2gn9kBTJ5Bn9kBTBLCDiQxkLDbvsP2b23vsP3oINrQxvZO2y83y1APdH26Zg29/ba3jdl2se2Ntrc338ddY29AbRuKZbwLy4wP9NgNevnzSf/MbnuKpN9J+oqk3ZJelLQyIl6d1Ia0sL1T0tKIGPgNGLb/SNK7kv7l9NJatv9e0sGIeLz5j3JGRPz1kLTtMX3KZbz71La2Zcb/TAM8dr1c/rwTgziz3yBpR0T8PiI+kPQTSXcNoB1DLyKel3TwY5vvkrS2ebxWo/9YJl1L24ZCROyJiM3N4yOSTi8zPtBjV2jXpBhE2OdLenPMz7s1XOu9h6Rf2n7J9upBN2Ycc8Yss7VX0pxBNmYc1WW8J9PHlhkfmmPXyfLn3eIC3SfdEhF/KOlPJH2rebs6lGL0M9gw9Z1+X9IXNLoG4B5J3xtkY5plxp+R9HBEHB5bG+SxG6ddk3LcBhH2EUmXjfn5c822oRARI833/ZLWafRjxzDZd3oF3eb7/gG35/9ExL6IOBkRpyT9QAM8ds0y489I+nFEPNtsHvixG69dk3XcBhH2FyUtsr3Q9tmSviFp/QDa8Qm2pzUXTmR7mqSvaviWol4v6f7m8f2SfjbAtnzEsCzj3bbMuAZ87Aa+/HlETPqXpDs1ekX+DUl/M4g2tLTr85L+p/l6ZdBtk/S0Rt/WfajRaxurJM2U9Jyk7ZL+U9LFQ9S2f9Xo0t5bNRqsuQNq2y0afYu+VdKW5uvOQR+7Qrsm5bhxuyyQBBfogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wUVU/7qrfcCsAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 0, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "print(images.max())\n",
    "#print(images)\n",
    "print(invTrans(images))\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 5, 8, 9, 13, 16, 17, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dcbd51-01e7-4670-9290-0a6d4f80e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import segmentation\n",
    "from pytorch_grad_cam import XGradCAM, GradCAM, FullGrad, GradCAMPlusPlus, ScoreCAM, AblationCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from skimage.segmentation import slic, felzenszwalb, quickshift, watershed, flood\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43085c17-7287-4bd2-9a32-43fb2384b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grayscale_grad_cam(image, SMU_class_index):\n",
    "    input_tensor = image.to(device)\n",
    "    targets = [ClassifierOutputTarget(SMU_class_index)]\n",
    "    #target_layers = [model.layer4[-1]]\n",
    "    target_layers = [model.layer2]\n",
    "    cam = GradCAM(model=model, target_layers=target_layers)\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "    \n",
    "    return(grayscale_cam)\n",
    "\n",
    "def segmentation_info(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "# I want to get the average attribution score for each segment\n",
    "def cam_processor_for_segments(grayscale_cam_output, segments_slic):\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    region_attr_score = []\n",
    "    final_region_attr_score = []\n",
    "    num_pixels_in_region_list = []\n",
    "    \n",
    "    for i in (list_unique_regions):\n",
    "        row_counter = 0\n",
    "        column_counter = 0\n",
    "        region_attr_score = []\n",
    "        num_pixels_in_region = 0\n",
    "        for row in grayscale_cam_output:\n",
    "            for cell in row:\n",
    "                current_score = grayscale_cam_output[row_counter, column_counter]\n",
    "                current_region = segments_slic[row_counter, column_counter]\n",
    "                if current_region == i:\n",
    "                    region_attr_score.append(current_score)\n",
    "                    num_pixels_in_region += 1\n",
    "                column_counter +=1\n",
    "            row_counter += 1\n",
    "            column_counter = 0\n",
    "        avg_score = np.mean(region_attr_score)\n",
    "        final_region_attr_score.append(avg_score)\n",
    "        num_pixels_in_region_list.append(num_pixels_in_region)\n",
    "    \n",
    "    unique_region_info = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(final_region_attr_score[i-1])\n",
    "        image_list.append(num_pixels_in_region_list[i-1])\n",
    "        image_list.append(np.sum(num_pixels_in_region_list))\n",
    "        unique_region_info.append(image_list)\n",
    "    \n",
    "    return(unique_region_info)\n",
    "\n",
    "\n",
    "def get_feature_masks(image, attributions, segments_slic):\n",
    "    segments_slic_1 = segments_slic\n",
    "    features = []\n",
    "    for i in attributions:\n",
    "        feature = np.where(i==segments_slic_1, 1, 0)\n",
    "        features.append(feature)\n",
    "        \n",
    "    return(features)\n",
    "\n",
    "\n",
    "def attribution_ranker(cam_processor_for_segments_output, num_top_attr):\n",
    "    ranked_images = sorted(cam_processor_for_segments_output, key=itemgetter(1), reverse=True)\n",
    "    top_ranked_features = []\n",
    "    for i in range(num_top_attr):\n",
    "        top_ranked_features.append(ranked_images[i][0])\n",
    "        \n",
    "    return top_ranked_features\n",
    "\n",
    "\n",
    "\n",
    "def image_rankings(get_image_versions):\n",
    "    #for idx in iterative_Grad_CAM_counterfactual_masking_output\n",
    "    ranked_images = sorted(get_image_versions, key=itemgetter(3))\n",
    "    \n",
    "    return ranked_images\n",
    "\n",
    "def blur_image_from_attribution(image, attribution_map):\n",
    "    # attribution map is the attributions after being passed through the attribution processor\n",
    "    # image is a tensor\n",
    "    # will output the blurred image based on the attribution map\n",
    "    \n",
    "    \n",
    "    #average_img = image.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "    #avg = np.average(average_img)\n",
    "    #blurred_img = cv2.GaussianBlur(image.squeeze().cpu().permute(1, 2, 0).numpy(), (181, 181), 0)\n",
    "    avg = np.float32(-1)\n",
    "    #avg_img = np.where(average_img > 9999, average_img, avg)\n",
    "    \n",
    "    #attribution_map = attribution_map.detach().squeeze().cpu().numpy()\n",
    "    \n",
    "    mask = [attribution_map]\n",
    "    mask = np.array(mask).squeeze()\n",
    "    #mask = mask.transpose(1,2,0)\n",
    "    #print(mask.shape)\n",
    "    #print(image.squeeze().cpu().numpy().shape)\n",
    "    \n",
    "    \n",
    "    out = np.where(mask==np.array([0]), image.squeeze().cpu().numpy(), avg)\n",
    "    #out = np.where(mask==np.array([0, 0, 0]), image.squeeze().cpu().permute(1, 2, 0).numpy(), blurred_img)\n",
    "    \n",
    "    out = torch.tensor(out)\n",
    "    out = out\n",
    "    out = out.unsqueeze(0)\n",
    "    out = out.unsqueeze(0)\n",
    "    #print(out.shape)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bfe8a8e-6985-4874-beae-7986815658f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_info_slic(image, num_segments, compactness):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = slic(img_np, n_segments = num_segments, compactness=compactness,\n",
    "                     start_label=1)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "def segmentation_info_felzenszwalb(image, scale, sigma, min_size):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = felzenszwalb(img_np, scale=scale, sigma=sigma, min_size=min_size)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "def segmentation_info_quickshift(image, kernel_size, max_dist, ratio):\n",
    "    img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = quickshift(img_np, kernel_size, max_dist, ratio)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "def segmentation_info_bass(segmentation_dir):\n",
    "    #img_np = image.detach().cpu().squeeze().numpy()\n",
    "    segments_slic = quickshift(img_np, kernel_size, max_dist, ratio)\n",
    "    num_segments = len(np.unique(segments_slic))\n",
    "    list_unique_regions = np.unique(segments_slic)\n",
    "    segment_pixel_num_list = []\n",
    "    total_pixels = 0\n",
    "    for i in (list_unique_regions):\n",
    "        num_pixels = np.count_nonzero(segments_slic == i)\n",
    "        segment_pixel_num_list.append(num_pixels)\n",
    "        total_pixels += num_pixels\n",
    "    \n",
    "    \n",
    "    information_for_each_segment = []\n",
    "    for i in (list_unique_regions):\n",
    "        image_list = []\n",
    "        image_list.append(i)\n",
    "        image_list.append(segment_pixel_num_list[i-1])\n",
    "        image_list.append(total_pixels)\n",
    "        information_for_each_segment.append(image_list)\n",
    "\n",
    "    return(information_for_each_segment, segments_slic, num_segments)\n",
    "\n",
    "\n",
    "def softmax_score(num_total_pixels, num_obf_pixels, model, image, SMU_class_index):\n",
    "    #image = good_img_transform(image)\n",
    "    image = image\n",
    "    logits = model(image).cpu()\n",
    "    #print(logits)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    probs = probs[SMU_class_index]\n",
    "\n",
    "    return probs\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index, threshold, top_n_stop):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "    print(len(features_1))\n",
    "    \n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    best_score_other_class = 0\n",
    "    \n",
    "    previous_features = False\n",
    "    powerset_list = list()\n",
    "    start = perf_counter()\n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "\n",
    "\n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        # print(type(features_list[-1]))\n",
    "        if previous_features:\n",
    "            powerset_list = np.array([list(ele) for ele in powerset_list if len(ele) != 0], dtype=object)\n",
    "            for ele in powerset_list:\n",
    "                for i, mask in enumerate(ele[:]):\n",
    "                    if np.array_equal(mask, features_list[-1]):\n",
    "                        ele = np.delete(mask, i, axis=0)\n",
    "        else:\n",
    "            powerset_list = [list(ele) for ele in powerset_list if len(ele) != 0]\n",
    "        \n",
    "        \n",
    "        # print(len(powerset_list))\n",
    "        \n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "    \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            \n",
    "            if (sm_idx1 != sm_idx2) and sm2[sm_idx2] > best_score_other_class:\n",
    "                best_score_other_class = sm2[sm_idx2] \n",
    "            \n",
    "            if (sm_idx1 != sm_idx2) and (sm2[sm_idx2] > threshold):\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "        if top_n == top_n_stop:\n",
    "            end = perf_counter() - start\n",
    "            print(f'Total Search time: {end:.3f}')\n",
    "            return -1\n",
    "        previous_features = True\n",
    "    \n",
    "    end = perf_counter() - start\n",
    "    print(f'Total Search time: {end:.3f}')\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa1a54de-3080-4618-bef6-62059c120e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6])\n",
      "tensor([6], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f34350404f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 26, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 0, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e68775b9-cda0-47e9-ae96-cb31617d6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 7\n",
      "index: 9\n",
      "index: 12\n",
      "index: 16\n",
      "index: 20\n",
      "index: 58\n",
      "index: 62\n",
      "index: 73\n",
      "index: 78\n",
      "index: 92\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "n = 0\n",
    "while i < 10:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    \n",
    "    if just_label == 9 and predicted == 9:\n",
    "        print('index:', n)\n",
    "        i += 1\n",
    "    \n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64475f9b-7cbe-47a5-8a92-0a9ad2b81118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9])\n",
      "tensor([9], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f57941c1fd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 58, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens: 5, 8, 9, 13, 16, 17, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73904f2-f297-44c2-b772-95921e51ed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 6  6  6  6  6  6  2  2  2  2  2  2  3  3  3  3  3  3  7  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  6  9  9  9  9  9  9  3  3  3  3  3  7  7  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  6  9  9  9  9  9  9 10 10 10 10 10 10  7  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  6  9  9  9  9  9 10 10 10 10 10 10 10 10  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  9  9  9  9  9  9  9 11 11 11 11 10 10 10  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [ 6  6  6  6  6  9  9  9  9  9  9 11 11 11 11 11 11 10 10  7  7  7  7  8\n",
      "   8  8  8  8]\n",
      " [12 12 12 12 12  9  9 13 13 13 13 11 11 11 11 11 11 14 14 14  7 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 11 11 11 11 11 11 14 14 14 15 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 11 11 11 11 11 14 14 14 15 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 11 11 11 11 11 14 14 14 15 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 11 11 11 11 11 14 14 14 16 15 15 15\n",
      "  15 15 15 15]\n",
      " [12 12 12 12 12 12 13 13 13 13 13 13 17 17 17 17 17 14 14 16 16 16 16 15\n",
      "  15 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 17 14 14 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 17 14 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 17 21 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 17 17 17 17 17 21 21 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 19 19 20 20 20 20 20 20 17 17 17 17 21 21 16 16 16 16 16 18\n",
      "  18 18 18 18]\n",
      " [19 19 19 19 22 22 20 20 23 23 23 23 23 17 17 21 21 21 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 21 21 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 21 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 21 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [22 22 22 22 22 22 22 23 23 23 23 23 23 23 21 21 24 24 24 24 24 24 24 25\n",
      "  25 25 25 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f57945a2b20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "print(segments_slic)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f34c216e-cdc1-43b5-abe0-e7c33c5709a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5794511340>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMfElEQVR4nO3dTahc9R3G8eeptRt1kVR7DRqqKW5EqDaDFipFEcW6iW7ELKpF6XWhoFBoxSIKUgh9xVUhVjGKVQQVRaRqozTtJjqRVKO2amPEhGsSm4W6suqvizmRa5y3zHm98/t+4DIz58w95zf/3Cfn5T/n/B0RAjD/vtZ2AQCaQdiBJAg7kARhB5Ig7EASX29yZbY7e+p//frZf3fHjurqmEWXa+9ybWWU+VxSvZ8tIjxsust0vdm+VNJdko6R9KeI2DTh/Z0Ne5keSA9t2uZ0ufYu11ZG2R7rOj9b5WG3fYykNyVdLGmvpJckbYyI18f8DmGvQZdr73JtZazEsJc5Zj9X0tsRsTsiPpH0sKQNJZYHoEZlwn6KpPeWvd5bTPsS24u2+7b7JdYFoKTaT9BFxGZJm6Vu78YD867Mln2fpLXLXp9aTAPQQWXC/pKkM2yfbvsbkq6S9GQ1ZQGo2sy78RHxqe0bJT2jQdfbvRHxWmWVAahUqX72o15Zh4/ZV3IXUZdr73JtZWTregOwghB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxMxDNs9i/Xqp329yjc1ocCDcVOa5Xev6bL3e6Hmlwm57j6SPJH0m6dOIGLMqAG2qYst+YUR8UMFyANSIY3YgibJhD0nP2t5he3HYG2wv2u7b7h88WHJtAGZWdjf+/IjYZ/tbkp6z/a+I2Lb8DRGxWdJmSer1PMenXIBuK7Vlj4h9xeMBSY9LOreKogBUb+aw2z7O9gmHn0u6RNKuqgoDUK0yu/ELkh63fXg5f46Iv1RSFVaMee4Lnzczhz0idkv6boW1AKgRXW9AEoQdSIKwA0kQdiAJwg4k4Wiw78Tu7jfoyjTDoPexm9ruGuty25RRtl3rbJeIGLp0tuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kESjt5JGPZ5//vkxcy+c8Nv1doSX+R7H1VdfPXb+Aw88MPOyM2LLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcD17oc3r2Xfv3j12/umnryu3gs4q9+fgFi+W53p2AJ1F2IEkCDuQBGEHkiDsQBKEHUiCsANJNNrP3us5+v3GVvclk/o1276/elvq7qou167jf/m8884bO//FF18ss/Kx5rKf3fa9tg/Y3rVs2mrbz9l+q3hcVWWxAKo3zW78fZIuPWLaLZK2RsQZkrYWrwF02MSwR8Q2SYeOmLxB0pbi+RZJl1dbFoCqzXqCbiEilorn70taGPVG24u2+7b7Bw/OuDYApZU+Gx+DM3wjT1dExOaI6EVE76STyq4NwKxmDft+22skqXg8UF1JAOowa9iflHRN8fwaSU9UUw6Auky8b7zthyRdIOlE23sl3S5pk6RHbF8n6V1JV9ZZZBVWcj/6Sv6OwLjaJ9c9/oNv337U5XRGXf9mvd7oeRPDHhEbR8y6aMZ6ALSAr8sCSRB2IAnCDiRB2IEkCDuQRKNDNu/YUf8llaPU2T3V4h2NgamxZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJBrtZ2/TSr5MFKOM/0erc0jnLt9KehS27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSmBh22/faPmB717Jpd9jeZ3tn8XNZvWUCKGuaLft9ki4dMv0PEXF28fN0tWUBqNrEsEfENkmHGqgFQI3KHLPfaPuVYjd/1ag32V603bfdL7EuACU5prhznu3TJD0VEWcVrxckfaDBHf/ulLQmIq6dYjmdva1jmRsItj2w40qtvfxNPrnh5DARMXTpM23ZI2J/RHwWEZ9LulvSuWWKA1C/mcJue82yl1dI2jXqvQC6YeJ9420/JOkCSSfa3ivpdkkX2D5bg/2oPZKun2Zl69dL/Tk8cl/J95xfybVL4/eFu/zZ6qqt1xs9b2LYI2LjkMn3lKgHQAv4Bh2QBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lMdaeaylbGnWpqsVJr7/LdXibpcu2V3qkGwMpD2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lMHMUVKOO2224bM/fOxurAFFt222ttv2D7dduv2b6pmL7a9nO23yoeV9VfLoBZTbMb/6mkn0XEmZK+L+kG22dKukXS1og4Q9LW4jWAjpoY9ohYioiXi+cfSXpD0imSNkjaUrxti6TLa6oRQAWO6pjd9mmSzpG0XdJCRCwVs96XtDDidxYlLZaoEUAFpj4bb/t4SY9KujkiPlw+LwZ3rRx6C76I2BwRvYjolaoUQClThd32sRoE/cGIeKyYvN/2mmL+GkkH6ikRQBUm3kratjU4Jj8UETcvm/4bSf+NiE22b5G0OiJ+PmFZ3Eq6Bl2uvUxt77yze+z8devWzb7wklbiraSnOWb/gaQfS3rV9s5i2q2SNkl6xPZ1kt6VdGUFdQKoycSwR8Q/JI36f+iiassBUBe+LgskQdiBJAg7kARhB5Ig7EASXOKKzmqzH71udY2U3hvzPVW27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRKP97OvXS/1+k2tsRl19pivBySefPOEd78+87MztWge27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBNezo5Slpdn70edZ22MJDMOWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSmGZ89rWS7pe0ICkkbY6Iu2zfIemnkg4Wb701Ip6esKzOXqHc5THOy+jyNeHz3G5tfrZR47NPE/Y1ktZExMu2T5C0Q9LlGozH/nFE/HbaIgh78wj7bOYx7NOMz74kaal4/pHtNySdUm15AOp2VMfstk+TdI6k7cWkG22/Yvte26tG/M6i7b7tObwhFbByTNyN/+KN9vGS/ibpVxHxmO0FSR9ocBx/pwa7+tdOWEZndyrZjW/ePLdbF3fjpwq77WMlPSXpmYj4/ZD5p0l6KiLOmrCczv7pEfbmzXO7dTHsE3fjbVvSPZLeWB704sTdYVdI2lW2SAD1meZs/PmS/i7pVUmfF5NvlbRR0tka7MbvkXR9cTJv3LI6u52Z1y37JG1u+bvcbvO4ZZ/6mL0KhL17CPtw8xh2vkEHJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Ioukhmz+Q9O6y1ycW01o35JLEztR2hErrqvhSzK62mXSUtTV8iWqV7fbtUTMavZ79Kyu3+xHRa62AMbpaW1frkqhtVk3Vxm48kARhB5JoO+ybW17/OF2trat1SdQ2q0Zqa/WYHUBz2t6yA2gIYQeSaCXsti+1/W/bb9u+pY0aRrG9x/artne2PT5dMYbeAdu7lk1bbfs5228Vj0PH2Guptjts7yvabqfty1qqba3tF2y/bvs12zcV01ttuzF1NdJujR+z2z5G0puSLpa0V9JLkjZGxOuNFjKC7T2SehHR+pdDbP9Q0seS7j88tJbtX0s6FBGbiv8oV0XELzpS2x06ymG8a6pt1DDjP1GLbVfl8OezaGPLfq6ktyNid0R8IulhSRtaqKPzImKbpENHTN4gaUvxfIsGfyyNG1FbJ0TEUkS8XDz/SNLhYcZbbbsxdTWijbCfIum9Za/3qlvjvYekZ23vsL3YdjFDLCwbZut9SQttFjPExGG8m3TEMOOdabtZhj8vixN0X3V+RHxP0o8k3VDsrnZSDI7ButR3+kdJ39FgDMAlSb9rs5himPFHJd0cER8un9dm2w2pq5F2ayPs+yStXfb61GJaJ0TEvuLxgKTHNTjs6JL9h0fQLR4PtFzPFyJif0R8FhGfS7pbLbZdMcz4o5IejIjHismtt92wuppqtzbC/pKkM2yfbvsbkq6S9GQLdXyF7eOKEyeyfZykS9S9oaiflHRN8fwaSU+0WMuXdGUY71HDjKvltmt9+POIaPxH0mUanJH/j6RftlHDiLrWSfpn8fNa27VJekiD3br/aXBu4zpJ35S0VdJbkv4qaXWHantAg6G9X9EgWGtaqu18DXbRX5G0s/i5rO22G1NXI+3G12WBJDhBByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/B8u/EWtwzP8oAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "print(img_np.shape)\n",
    "segments_slic = watershed(img_np, markers=25, compactness=0.001)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fe1c72d-737b-42ab-8236-4db6547ab334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f578d799bb0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = felzenszwalb(img_np, scale=5, sigma=0.5, min_size=5)\n",
    "#print(segments_slic)\n",
    "print(len(np.unique(segments_slic)))\n",
    "print(np.unique(segments_slic))\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9699755-2fa8-4950-a5c7-ce00399f9c54",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the input array must have size 3 along `channel_axis`, got (28, 28, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20864/3152609198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plt.imshow(img_np)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compactness=50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msegments_slic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquickshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(segments_slic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_slic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/segmentation/_quickshift.py\u001b[0m in \u001b[0;36mquickshift\u001b[0;34m(image, ratio, kernel_size, max_dist, return_tree, sigma, convert2lab, random_seed, channel_axis)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only RGB images can be converted to Lab space.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2lab\u001b[0;34m(rgb, illuminant, observer, channel_axis)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mStandard_illuminant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \"\"\"\n\u001b[0;32m-> 1139\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mxyz2lab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milluminant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/_shared/utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36mrgb2xyz\u001b[0;34m(rgb, channel_axis)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;31m# Follow the algorithm from http://www.easyrgb.com/index.php\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# except we don't multiply/divide by 100 in the conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_colorarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.04045\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1.055\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/skimage/color/colorconv.py\u001b[0m in \u001b[0;36m_prepare_colorarray\u001b[0;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[1;32m    138\u001b[0m         msg = (f'the input array must have size 3 along `channel_axis`, '\n\u001b[1;32m    139\u001b[0m                f'got {arr.shape}')\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mfloat_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_supported_float_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the input array must have size 3 along `channel_axis`, got (28, 28, 1)"
     ]
    }
   ],
   "source": [
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = quickshift(img_np, kernel_size=1, max_dist=6, ratio=0.5)\n",
    "#print(segments_slic)\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, segments_slic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6be1949-6963-4c67-9ccd-a1525bff5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_mask_to_numpy(csv_path: str) -> np.ndarray:\n",
    "    \"\"\"Converts a csv_mask_file into a numpy arrays.\n",
    "    \n",
    "    Args:\n",
    "        csv_mask_dir_path (str): The path to csv mask directory.\n",
    "            Note, this should be the same as 'output_dir_path' from run_bass() function\n",
    "        \n",
    "    Returns:\n",
    "        A list of np array masks.\n",
    "    \"\"\"\n",
    "    # - 1 so that the superpixel values start at 0\n",
    "    return np.loadtxt(csv_path, delimiter=\",\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501a0929-e6b2-4cde-9443-5494e3dad4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv\n"
     ]
    }
   ],
   "source": [
    "a = \"test.png\"\n",
    "b = a[:-3] + 'csv'\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "114db533-d541-4d36-972c-3943b9ccdce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0da49ce580>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_dir = \"../labelme/MNIST_71/test_images\"\n",
    "img = Image.open(str(data_dir)+\"/seven_170.png\")\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        # transforms.RandomAffine(20, translate=(0.20,0.20))\n",
    "    ])\n",
    "image_to_tensor = transform_test(img).unsqueeze(0)\n",
    "img_np = image_to_tensor.detach().cpu().squeeze().numpy()\n",
    "bass_seg = csv_mask_to_numpy(\"../labelme/MNIST_71/BASS_output/seven_170.csv\")\n",
    "plt.imshow(segmentation.mark_boundaries(img_np, bass_seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ec91c4a-f5a8-47df-880e-fa8ef3e2f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working_example = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "079ce5a4-26e3-4d45-ac78-d2b727281400",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'working_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13967/3530658002.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_example\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'working_example' is not defined"
     ]
    }
   ],
   "source": [
    "print(working_example[-2])\n",
    "print(working_example[-1])\n",
    "print(model(images.to(device)))\n",
    "print(model(working_example[0].to(device)))\n",
    "plt.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39d1964c-c910-479b-89f9-dc56a3a40c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([7], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0da517baf0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "#images, labels = next(itertools.islice(testloader, 48, None))\n",
    "images, labels = next(itertools.islice(test_data, 60, None))\n",
    "\n",
    "print(labels)\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = outputs.max(1)\n",
    "print(predicted)\n",
    "pred_val = predicted.item()\n",
    "plt.imshow( images.detach().cpu().squeeze(), cmap='gray' )\n",
    "\n",
    "# Good sevens:index: 0, 17, 26, 34, 36, 41, 60, 64, 70, 75\n",
    "# Good outputs 17, 48, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc66067e-f097-45ba-9716-46abc5f87595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14731/1627894948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mClassifierOutputTarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "input_tensor = images.to(device)\n",
    "print(input_tensor.shape)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "print(targets)\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.min())\n",
    "plt.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "036cbdfd-0c83-462d-81a8-300ef9117087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 339.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0da5c17700>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(7)]\n",
    "target_layers = [model.layer2]\n",
    "cam = AblationCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "print(grayscale_cam.min())\n",
    "plt.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79a2508-c955-4c57-8d5e-2be0f67c98aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Regions analyzed 1\n",
      "Regions analyzed 2\n",
      "Regions analyzed 3\n",
      "Regions analyzed 4\n",
      "Regions analyzed 5\n",
      "Regions analyzed 6\n",
      "Regions analyzed 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regions analyzed 8\n",
      "Total Search time: 0.319\n",
      "regions analyzed 8\n",
      "Original Version Predicted Class: 6    With Confidence: 0.7592856\n",
      "Modified Version Predicted Class: 4    With Confidence: 0.9735357\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1008x1008 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Just give an images and it will output\n",
    "\n",
    "input_tensor = images.to(device)\n",
    "targets = [ClassifierOutputTarget(6)]\n",
    "target_layers = [model.layer2]\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "inv_img = images\n",
    "img_np = inv_img.detach().cpu().squeeze().numpy()\n",
    "#plt.imshow(img_np)\n",
    "# compactness=50\n",
    "segments_slic = slic(img_np, n_segments=25, compactness=1,\n",
    "                     start_label=1)\n",
    "\n",
    "working_example = region_explainability(image = images, top_n_start = 1, \n",
    "                                        model = model, SMU_class_index = 6, \n",
    "                                        threshold = 0.8, top_n_stop = 21)\n",
    "print(\"regions analyzed\", working_example[-2])\n",
    "sm1 = softmax(model(images.to(device)).cpu().detach().numpy()).squeeze()\n",
    "sm_idx1 = np.argmax(sm1)\n",
    "sm2 = softmax(model(working_example[0].to(device)).cpu().detach().numpy()).squeeze()\n",
    "sm_idx2 = np.argmax(sm2)\n",
    "print(\"Original Version Predicted Class:\",sm_idx1, \"   With Confidence:\", sm1[sm_idx1])\n",
    "print(\"Modified Version Predicted Class:\",sm_idx2, \"   With Confidence:\", sm2[sm_idx2])\n",
    "#print(sm2)\n",
    "\n",
    "#print(model(images.to(device)))\n",
    "#print(model(working_example[0].to(device)))\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n",
    "# ax1.imshow(images.detach().cpu().squeeze(), cmap='gray' )\n",
    "# plt.axis('off')\n",
    "# ax2.imshow(grayscale_cam, cmap='gray', vmin=0, vmax=1)\n",
    "# plt.axis('off')\n",
    "# ax3.imshow(segmentation.mark_boundaries(img_np, segments_slic))\n",
    "# plt.axis('off')\n",
    "# ax4.imshow(working_example[0].detach().cpu().squeeze(), cmap='gray')\n",
    "# plt.axis('off')\n",
    "\n",
    "plot_images = (input_tensor.detach().cpu().squeeze(),\n",
    "                  grayscale_cam,\n",
    "                  segmentation.mark_boundaries(img_np, segments_slic),\n",
    "                  working_example[0].detach().cpu().squeeze())\n",
    "    \n",
    "figure_name = plt.figure(figsize=(14, 14))\n",
    "for i, img in enumerate(plot_images):\n",
    "    plt.subplot(1, 4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.margins(x=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958bc42b-c1dd-4734-866b-800ff656ced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -0.9922, -1.0000, -1.0000, -0.1686,  0.0902,\n",
      "           -0.0902,  0.2549, -0.3098, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -0.4902,  1.0000,  0.8510,\n",
      "            0.7961,  0.8118,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -0.4588,  0.5294,  0.9059,  0.5451,  0.8196,\n",
      "            0.8980,  0.7882,  0.5137,  0.8667, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843, -1.0000,\n",
      "           -0.2235,  0.5137,  0.6784,  0.6157,  0.4980,  0.7255,  0.9373,\n",
      "            0.7255,  0.8745,  0.7020,  0.5294,  0.6471, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.9922, -1.0000, -0.7333,\n",
      "            0.6392,  0.5373,  0.4510,  0.4275,  0.5922,  0.6392,  0.5451,\n",
      "            0.5608,  0.6706,  0.6941,  0.5686, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1216,\n",
      "            0.6157,  0.4667,  0.5216,  0.4902,  0.4588,  0.4275,  0.4824,\n",
      "            0.4667,  0.4196,  0.4431,  0.4824, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7176,\n",
      "            0.5843,  0.5216,  0.4902,  0.4902,  0.5216,  0.4980,  0.5608,\n",
      "            0.6000,  0.5059,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7647,\n",
      "            0.5922,  0.5529,  0.5373,  0.5216,  0.5216,  0.4824,  0.5137,\n",
      "            0.4431,  0.4588,  0.4902, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5216,\n",
      "            0.6471,  0.6000,  0.5059,  0.4353,  0.3961,  0.4039,  0.4510,\n",
      "            0.4353,  0.4118,  0.3961,  0.3961, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7569,  0.6627,\n",
      "            0.6549,  0.5843,  0.5529,  0.4431,  0.3961,  0.4353,  0.5294,\n",
      "            0.4902,  0.3882,  0.4196,  0.4196, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5137,  0.7098,\n",
      "            0.7176,  0.8510,  0.5843,  0.6000,  0.5059,  0.4588,  0.5294,\n",
      "            0.3961,  0.4745,  0.4510,  0.5216, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1373,  0.7176,\n",
      "            0.7490,  0.8196,  0.5608,  0.6392,  0.4588,  0.4353,  0.5529,\n",
      "            0.4824,  0.5765,  0.4824,  0.5059, -1.0000, -1.0000,  0.6784,\n",
      "            0.6314,  0.7098,  0.3569, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.1608,  0.6863,\n",
      "            0.8039,  0.4431,  0.4980,  0.6471,  0.4431,  0.4588,  0.5765,\n",
      "            0.5529,  0.4039,  0.4980,  0.4667,  0.5608,  0.7961,  0.0431,\n",
      "            0.7804,  0.7020,  0.6784, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4431,  0.6549,\n",
      "            0.8902, -0.2627,  0.4353,  0.6706,  0.4353,  0.5216,  0.6157,\n",
      "            0.4353,  0.5137,  0.4980,  0.4902,  0.4824,  0.8667, -0.5137,\n",
      "            0.8824,  0.6941,  0.8588, -0.8588, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7412,  0.6471,\n",
      "            1.0000, -1.0000,  0.3804,  0.7020,  0.4196,  0.5137,  0.6392,\n",
      "            0.4196,  0.4902,  0.5294,  0.4745,  0.4980,  0.8588, -1.0000,\n",
      "            0.6471,  0.7725,  0.6471, -0.3176, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5608,  0.6941,\n",
      "            0.8902, -1.0000,  0.2863,  0.6784,  0.4588,  0.4667,  0.5765,\n",
      "            0.5216,  0.4588,  0.4745,  0.5765,  0.5294,  0.8118, -1.0000,\n",
      "            0.0745,  0.8667,  0.6157,  0.1608, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8902,  0.6392,  0.7882,\n",
      "            0.6941, -1.0000,  0.4588,  0.6392,  0.4902,  0.4980,  0.5451,\n",
      "            0.6000,  0.5529,  0.4275,  0.5294,  0.5451,  0.8980, -1.0000,\n",
      "           -0.3961,  1.0000,  0.6157,  0.3490, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.8510,  0.6471,  0.8275,\n",
      "            0.6000, -1.0000,  0.7098,  0.5843,  0.4588,  0.5216,  0.6078,\n",
      "            0.3882,  0.4902,  0.5059,  0.4196,  0.4118,  0.9843, -1.0000,\n",
      "           -0.5843,  1.0000,  0.6000,  0.3725, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.6549,  0.6471,  0.7647,\n",
      "            0.6000, -1.0000,  0.9137,  0.4902,  0.5216,  0.4745,  0.6078,\n",
      "            0.4824,  0.4824,  0.4745,  0.5216,  0.3804,  0.8588, -0.3647,\n",
      "           -0.5843,  1.0000,  0.6471,  0.4118, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.4824,  0.6706,  0.7255,\n",
      "            0.4980, -0.5843,  0.8588,  0.4118,  0.4667,  0.4824,  0.6314,\n",
      "            0.5922,  0.5137,  0.4745,  0.4588,  0.4196,  0.6392,  0.3098,\n",
      "           -0.5765,  1.0000,  0.7098,  0.1765, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.4824,  0.9137,  0.6863,\n",
      "            0.4980,  0.1294,  0.6863,  0.4431,  0.4667,  0.5529,  0.6000,\n",
      "            0.5137,  0.5373,  0.5216,  0.4588,  0.4431,  0.4980,  0.5529,\n",
      "           -0.0902,  1.0000,  0.8353, -0.0588, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.4667,  0.9216,  0.7098,\n",
      "            0.5137,  0.3882,  0.5765,  0.4824,  0.4824,  0.5373,  0.5922,\n",
      "            0.4510,  0.5137,  0.4824,  0.4745,  0.4510,  0.4980,  0.6000,\n",
      "            0.0745,  0.9294,  0.8039,  0.1843, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.3569,  0.9216,  0.6784,\n",
      "            0.5922,  0.4667,  0.5294,  0.4824,  0.4902,  0.4824,  0.5843,\n",
      "            0.5059,  0.5373,  0.4824,  0.4588,  0.4824,  0.4980,  0.5451,\n",
      "            0.2549,  0.8039,  0.7804,  0.1922, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -0.6000,  0.9529,  0.6863,\n",
      "            0.6627,  0.4667,  0.5059,  0.5137,  0.5137,  0.4980,  0.5843,\n",
      "            0.4902,  0.5373,  0.4824,  0.4824,  0.4902,  0.4980,  0.4588,\n",
      "            0.5137,  0.8039,  0.8353, -0.0275, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5686,  0.7020,\n",
      "            0.6314,  0.6392,  0.5373,  0.4745,  0.4824,  0.4745,  0.5059,\n",
      "            0.5137,  0.5137,  0.4588,  0.4510,  0.4510,  0.4824,  0.6157,\n",
      "            0.6235,  0.7725,  0.7490, -0.2392, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.3569,  0.7098,\n",
      "            0.4275,  0.1529,  0.8824,  0.6000,  0.5922,  0.6078,  0.6235,\n",
      "            0.6000,  0.6078,  0.6000,  0.5922,  0.5765,  0.8118,  0.4980,\n",
      "           -0.0588,  0.7882,  0.7176, -0.2784, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.6392,  0.8431,\n",
      "            0.4980, -1.0000, -1.0000, -0.6784, -0.4510, -0.3804, -0.3725,\n",
      "           -0.3255, -0.3255, -0.2706, -0.4196, -0.7490, -1.0000, -1.0000,\n",
      "           -0.6706,  0.8353,  0.7333, -0.1922, -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8510, -0.6471,\n",
      "           -0.8980, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "           -1.0000, -0.9451, -0.8196, -1.0000, -1.0000, -1.0000, -1.0000]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(working_example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f886cda-92c8-481a-82f5-e09414e6c08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 1  1  1  1  1  1  2  2  2  2  2  2  3  3  3  3  3  3  4  4  4  4  4  4\n",
      "   5  5  5  5]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7  7  3  3  3  3  3  8  8  8  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7  7  7  3 10 10 10 10 10  8  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7  7 10 10 10 10 10 10 10  8  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7  7  7 10 10 10 10 10 10 10 10 10  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7  7  7 11 11 11 10 12 12 12 12 10 10 13  8  8  8  9\n",
      "   9  9  9  9]\n",
      " [ 6  6  6  6  6  6  7 11 11 11 11 12 12 12 12 12 12 12 13 13 13  8  8  9\n",
      "   9  9  9  9]\n",
      " [14 14 14 14 14 14 11 11 11 11 12 12 12 12 12 12 12 12 13 13 13 13 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 11 11 11 12 12 12 12 12 12 12 12 13 13 13 13 15 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 11 11 11 11 12 12 12 12 13 13 13 13 13 13 13 15 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 14 11 11 11 11 11 11 11 13 13 13 13 13 13 13 15 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 14 11 11 11 11 11 11 11 13 13 13 13 13 13 16 16 15 15\n",
      "  15 15 15 15]\n",
      " [14 14 14 14 14 14 14 17 17 17 17 17 17 17 17 13 18 13 13 16 16 16 16 15\n",
      "  15 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 18 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 17 17 17 17 17 17 17 17 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [20 20 20 20 20 20 20 21 21 21 17 17 17 22 22 18 18 18 16 16 16 16 16 19\n",
      "  19 19 19 19]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 18 18 18 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]\n",
      " [23 23 23 23 23 23 21 21 21 21 21 21 22 22 22 22 22 24 24 24 24 24 24 25\n",
      "  25 25 25 25]]\n"
     ]
    }
   ],
   "source": [
    "print(segments_slic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964de447-2c49-4819-9783-95943389ecac",
   "metadata": {},
   "source": [
    "# Quantitative Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30cc76-24d6-4a87-808a-12501799d81a",
   "metadata": {},
   "source": [
    "## Experimenting with different clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c737dd-66be-48ea-8bfc-2f3ff778abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "    \n",
    "    sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "    sm_idx1 = np.argmax(sm1)\n",
    "    pred_class = sm1[sm_idx1]\n",
    "    pred = pred_class\n",
    "    \n",
    "    while pred == pred_class:\n",
    "    #while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            # score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "            #                       num_obf_pixels = num_pixels_changed,\n",
    "            #                       model = model,\n",
    "            #                       image = obfuscated_image,\n",
    "            #                       SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            # sm1 is softmax scores of original image, sm_idx1 is the index of top softmax score (the predicted class)\n",
    "            # sm1[sm_idx1] gives the softmax score of the predicted class\n",
    "            # sm2 is like sm1 but on an obfuscated image\n",
    "            # sm1 = softmax(model(image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            # sm_idx1 = np.argmax(sm1)\n",
    "            sm2 = softmax(model(obfuscated_image.to(device)).cpu().detach().numpy()).squeeze()\n",
    "            sm_idx2 = np.argmax(sm2)\n",
    "            if sm_idx1 != sm_idx2:\n",
    "                pred_class = sm_idx2\n",
    "                image_versions.append(obfuscated_image)\n",
    "                #sm2[sm_idx1] is the softmax score of the obfuscated image of the original class.\n",
    "                #This score shows us how far the prediction has changed from the original image\n",
    "                scores.append(sm2[sm_idx1])\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "            \n",
    "#             if score < 0.5:\n",
    "#                 prob = score\n",
    "#                 image_versions.append(obfuscated_image)\n",
    "#                 scores.append(score)\n",
    "#                 num_pixels_changed.append(num_changes)\n",
    "#                 total_attr_list.append(total_attribution)\n",
    "                \n",
    "#                 #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    top_n = top_n - 1\n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        image_list.append(avg_attr_scores)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ea514-1555-436c-aa39-76858a9cf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with SLIC with 25 regions and 1 compactness\n",
    "i = 0\n",
    "n = 0\n",
    "image_info_list = []\n",
    "while i < 50:\n",
    "    torch.manual_seed(0)\n",
    "    #testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    #images, labels = next(itertools.islice(testloader, n, None))\n",
    "    images, labels = next(itertools.islice(test_data, n, None))\n",
    "    just_label = labels.item()\n",
    "    \n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = outputs.max(1)\n",
    "    predicted = predicted.cpu().item()\n",
    "    #n += 1\n",
    "    #print()\n",
    "    #print(predicted)\n",
    "    \n",
    "    logits = model(images.to(device)).cpu()\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probs = probs.detach().cpu()\n",
    "    probs = probs.tolist()[0]\n",
    "    # Change probs[int] to int = SMU class index\n",
    "    probs_orig = probs[0]\n",
    "    #print(probs)\n",
    "    \n",
    "    \n",
    "    if just_label == 7 and predicted ==7:\n",
    "        print('index:', i+1)\n",
    "        i += 1\n",
    "        \n",
    "        re = region_explainability(image = images, top_n_start = 1, model = model, SMU_class_index = 7)\n",
    "        \n",
    "        \n",
    "        image_info = []\n",
    "        example = re[0]\n",
    "        exam_img = good_img_transform(example)\n",
    "        logits = model(exam_img).cpu()\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        probs = probs.detach().cpu()\n",
    "        probs = probs.tolist()[0]\n",
    "        # Change probs[int] to int = SMU class index\n",
    "        probs_obf = probs[0]\n",
    "        #print(probs)\n",
    "        image_info.append(probs_orig)\n",
    "        image_info.append(probs_obf)\n",
    "        \n",
    "        num_pixels_obf = re[1]\n",
    "        image_info.append(num_pixels_obf)\n",
    "        image_info.append(re[5])\n",
    "        sm1 = softmax(model(example.to(device)).cpu().detach().numpy()).squeeze()\n",
    "        sm_idx1 = np.argmax(sm1)\n",
    "        #sm_idx1 is the predicted class of the obfuscated image\n",
    "        image_info.append(sm_idx1)\n",
    "        \n",
    "        image_info_list.append(image_info)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b12456-0462-4376-ba6c-ce7a3fd69479",
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "total = len(image_info_list)\n",
    "total_pix = total * 28 * 28\n",
    "total_obf = 0\n",
    "total_orig_conf = 0\n",
    "total_obf_conf = 0\n",
    "total_regions = 0\n",
    "preds_on_images = []\n",
    "for i in range(len(image_info_list)):\n",
    "    total_orig_conf += image_info_list[i][0]\n",
    "    total_obf_conf += image_info_list[i][1]\n",
    "    total_obf += image_info_list[i][2]\n",
    "    total_regions += image_info_list[i][3]\n",
    "    preds_on_images.append(image_info_list[i][4])\n",
    "\n",
    "\n",
    "array_np = np.array(preds_on_images)\n",
    "unique, counts = np.unique(array_np, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "\n",
    "print(\"Experiment with SLIC with 25 segments and 1 compactness\")\n",
    "print(\"Average number of regions analyzed: \", total_regions / total)\n",
    "print(\"Average confidence change: \", ((total_orig_conf - total_obf_conf) / total) )\n",
    "print(\"Distribution of changed to class: \", dict(zip(unique, counts)))\n",
    "print(\"Average obfuscation: \",total_obf / total_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf22ea-15a9-4b6d-96b4-a2633eec562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To test our algorithm with felz with 25 regions and 1 compactness\n",
    "def region_explainability(image, top_n_start, model, SMU_class_index):\n",
    "    # Get attribution map\n",
    "    explainability_mask = get_grayscale_grad_cam(image,SMU_class_index)\n",
    "    # Get segment mask\n",
    "    seg = segmentation_info_slic(image = image, num_segments = 25, compactness = 1)\n",
    "    # Calculate average attribution in each superpixel\n",
    "    avg_attr_scores = cam_processor_for_segments(grayscale_cam_output = explainability_mask, segments_slic = seg[1])\n",
    "    # Sort the regions by average attribution, make num_top_attr = the number of segments in the image\n",
    "    top_attrs = attribution_ranker(cam_processor_for_segments_output = avg_attr_scores, num_top_attr = seg[2])\n",
    "    features_1 = get_feature_masks(image = image, attributions = top_attrs, segments_slic = seg[1])\n",
    "    # features_1 gives us a sorted list of feature masks. Element at position 0 is the top attribution region mask\n",
    "\n",
    "    top_n = top_n_start\n",
    "    score = 1000\n",
    "    prob = 1\n",
    "    \n",
    "    # The computational cost of this loop could be reduced by approximately half\n",
    "    # Currently I do a counterfactual analysis on top_n regions and expand top_n to top_n + 1\n",
    "    # This implementation has us redo the counterfactual analysis of the top_n when doing counterfactual analysis on top_n + 1\n",
    "\n",
    "    while prob > 0.5:\n",
    "        #image_versions holds the image with regions obfuscated\n",
    "        image_versions = []\n",
    "        #num_pixels_changed holds the count of the number of pixels that are obfuscated\n",
    "        num_pixels_changed = []\n",
    "        #total_attr_list I think gives us the label of the regions that are being obfuscated\n",
    "        total_attr_list = []\n",
    "        #scores holds the score given to the image with regions obfuscated\n",
    "        scores = []\n",
    "        \n",
    "        # features_list contains the features to be analyzed in counterfactual analysis\n",
    "        # features_list will start with the top 1 region and then go on to top 2 and so on\n",
    "        features_list = features_1[0:top_n]\n",
    "        \n",
    "        powerset_list = list(more_itertools.powerset(features_list))\n",
    "        powerset_list = [list(ele) for ele in powerset_list]\n",
    "        num_versions = len(powerset_list)\n",
    "        \n",
    "        #print(image.shape)\n",
    "        \n",
    "        original_image = invTrans(image)\n",
    "        \n",
    "        #print(original_image.shape)\n",
    "        \n",
    "        # image_versions.append(original_image)\n",
    "        # num_pixels_changed.append(0)\n",
    "        # total_attr_list.append(np.zeros((28, 28)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for version in range(num_versions - 1):\n",
    "            obfuscated_image = image\n",
    "            total_attribution = np.zeros((28, 28))\n",
    "            total_num_pixels = total_attribution.size\n",
    "            for mask in range(len(powerset_list[version + 1])):\n",
    "                total_attribution += powerset_list[version + 1][mask]\n",
    "            num_changes = np.count_nonzero(total_attribution)\n",
    "            obfuscated_image = blur_image_from_attribution(image = obfuscated_image,\n",
    "                                                       attribution_map = total_attribution)\n",
    "            obfuscated_image = obfuscated_image.to(device)\n",
    "            #obfuscated_image = invTrans(obfuscated_image)\n",
    "        \n",
    "            # calculate softmax score of obfuscated image on the unsafe image class\n",
    "            score = softmax_score(num_total_pixels = total_num_pixels,\n",
    "                                  num_obf_pixels = num_pixels_changed,\n",
    "                                  model = model,\n",
    "                                  image = obfuscated_image,\n",
    "                                  SMU_class_index = SMU_class_index)\n",
    "            #print(score)\n",
    "            \n",
    "            # if softmax score is less than 0.5, we want to save it as a counterfactual example\n",
    "            if score < 0.5:\n",
    "                prob = score\n",
    "                image_versions.append(obfuscated_image)\n",
    "                scores.append(score)\n",
    "                num_pixels_changed.append(num_changes)\n",
    "                total_attr_list.append(total_attribution)\n",
    "                \n",
    "                #print(score)\n",
    "        \n",
    "        print(\"Regions analyzed\", top_n)\n",
    "        top_n = top_n + 1\n",
    "    \n",
    "    \n",
    "    # Creating an array to hold the information with each counterfactual image we generated\n",
    "    # It is possible that we could have just one counterfactual image\n",
    "    unique_image_info = []\n",
    "    for i in range(len(scores)):\n",
    "        image_list = []\n",
    "        image_list.append(image_versions[i])\n",
    "        image_list.append(num_pixels_changed[i])\n",
    "        image_list.append(total_num_pixels)\n",
    "        image_list.append(scores[i])\n",
    "        image_list.append(total_attr_list[i])\n",
    "        image_list.append(top_n)\n",
    "        unique_image_info.append(image_list)\n",
    "    \n",
    "    \n",
    "    # Rank the different counterfactual images\n",
    "    ranked_images = image_rankings(get_image_versions = unique_image_info)\n",
    "    \n",
    "    # Get the best ranked image\n",
    "    best_masked_image = ranked_images[0]\n",
    "    \n",
    "    return best_masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96db9528-ab90-4a3c-b634-f49274165c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "56d22867d346137a858af224635421daa75b8ca5dd3197c9bf304264b2fbc7e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
